# Grundregeln fÃ¼r KI-Code-Audits (Cursor)

**Projekt:** FAMO TrafficApp 3.0  
**Version:** 1.0  
**Datum:** 2025-11-14

---

## Einleitung

Dieses Dokument definiert die Grundregeln fÃ¼r alle Code-Audits, die von Cursor AI durchgefÃ¼hrt werden. Ziel ist es, strukturierte, reproduzierbare und ganzheitliche Audits zu gewÃ¤hrleisten, die Backend, Frontend, Datenbank und Infrastruktur gleichermaÃŸen berÃ¼cksichtigen.

---

## 1. Scope immer explizit machen

**Zu Beginn jedes Audits klÃ¤ren:**

- Welches Feature / welcher Endpoint / welches UI-Element ist betroffen?
- Welche Symptome liegen vor (Fehlermeldungen, Logs, Screenshots)?
- Welche User Story oder welcher Bug-Report liegt zugrunde?

**Dokumentation:**

```md
## Audit-Scope

- **Feature:** Sub-Routen-Generator
- **Betroffene Endpoints:** `/api/tour/optimize`, `/api/subroutes/generate`
- **Symptome:** 500 Internal Server Error, TypeError in Browser-Konsole
- **Reproduktion:** Button "Routen optimieren" â†’ Fehler nach 3 Sekunden
```

---

## 2. Immer ganzheitlich prÃ¼fen

### 2.1 Backend (Python/FastAPI)

- **Routen:** `routes/`, `backend/routes/`
- **Services:** `services/`, `backend/services/`
- **Datenbank-Zugriff:** `db/`, `repositories/`
- **Modelle:** `backend/models/`, Pydantic-Schemas
- **Konfiguration:** `config.env`, `backend/config.py`

**PrÃ¼fungen:**

- Exception-Handling (try-catch, Error-Responses)
- Logging (strukturierte Logs mit Kontext)
- Input-Validierung (Pydantic, manuelle Checks)
- Timeouts bei externen Aufrufen (OSRM, LLM-APIs)

### 2.2 Frontend (HTML, CSS, JavaScript)

- **Entry Points:** `frontend/*.html`
- **JavaScript:** `frontend/js/*.js`, Inline-Scripts
- **API-Calls:** Alle `fetch()` Aufrufe
- **Event-Handler:** Button-Clicks, Form-Submissions

**PrÃ¼fungen:**

- Request/Response-Kontrakt mit Backend
- Fehlerbehandlung (catch-BlÃ¶cke, UI-Feedback)
- Defensive Programmierung (Null-Checks, Array-Validierung)
- Browser-Konsole auf Fehler prÃ¼fen

### 2.3 Datenbank (SQLite)

- **Schema:** `db/schema.py`, `db/migrations/*.sql`
- **Daten:** `data/traffic.db`, `data/customers.db`
- **Queries:** SQL-Statements in Services und Repositories

**PrÃ¼fungen:**

- Schema-Konsistenz (Code vs. reale DB)
- Migrationen (ALTER TABLE, CREATE INDEX)
- Indizes (Performance bei groÃŸen Tabellen)
- Datenkonsistenz (Constraints, Foreign Keys)

### 2.4 Infrastruktur / Externe Dienste

- **OSRM:** Docker-Container, Endpoints, Timeouts
- **LLM-APIs:** OpenAI, Ollama (falls genutzt)
- **Konfiguration:** ENV-Variablen, Ports, URLs
- **Health-Checks:** `/health/osrm`, `/api/osrm/metrics`

**PrÃ¼fungen:**

- Erreichbarkeit (Ping, Health-Endpoints)
- Timeouts und Retry-Logic
- Fehlerbehandlung bei Ausfall externer Dienste

---

## 3. Keine isolierten Fixes

**Regel:**

- Niemals nur eine einzelne Datei Ã¤ndern, ohne zu prÃ¼fen, wo sie Ã¼berall verwendet wird.
- Immer nach Seiteneffekten suchen (z.B. geÃ¤nderte Response-Formate â†’ Frontend anpassen).

**Vorgehen:**

1. **Grep/Search:** Finde alle Verwendungen der geÃ¤nderten Funktion/Klasse/API
2. **Impact-Analyse:** Welche anderen Module sind betroffen?
3. **Kontrakt-PrÃ¼fung:** Ã„ndert sich ein API-Kontrakt (Request/Response)?
4. **Tests anpassen:** Schlagen existierende Tests fehl?

**Beispiel:**

```python
# Backend: Response-Format geÃ¤ndert
# VORHER:
return {"subRoutes": [...]}

# NACHHER:
return {"sub_routes": [...]}  # snake_case statt camelCase

# â†’ Frontend MUSS angepasst werden!
# â†’ Alle Tests, die dieses Format erwarten, mÃ¼ssen angepasst werden!
```

---

## 4. Tests sind Pflicht

**FÃ¼r jeden Bugfix:**

- Mindestens einen **Regressionstest** vorschlagen (und idealerweise anlegen)
- Der Test soll sicherstellen, dass der konkrete Fehler nicht zurÃ¼ckkommt

**Test-Kategorien:**

1. **Unit Tests:** Einzelne Funktionen/Services testen
2. **Integration Tests:** API-Endpoints End-to-End testen
3. **Frontend Tests:** UI-Interaktionen testen (optional: Playwright)

**Beispiel:**

```python
# Test fÃ¼r Sub-Routen-Generator Bug
def test_subroutes_generator_with_w_tours():
    """
    Regression-Test fÃ¼r Bug #XYZ:
    Sub-Routen-Generator wirft TypeError bei W-Touren mit >4 Kunden
    """
    payload = {
        "tours": [
            {"key": "W07", "customers": ["Kunde1", "Kunde2", "Kunde3", "Kunde4", "Kunde5"]}
        ]
    }
    response = client.post("/api/tour/optimize", json=payload)
    assert response.status_code == 200
    data = response.json()
    assert "sub_routes" in data
    assert isinstance(data["sub_routes"], list)
```

---

### ğŸ¯ Golden Test Cases (fÃ¼r kritische Features)

**Zweck:** Kugelsicherer Modus fÃ¼r kritische Workflows

**Golden Tests sind:**
- Referenz-TestfÃ¤lle mit bekanntem, erwartetem Output
- MÃ¼ssen IMMER gleich bleiben (deterministisch)
- Decken reale, produktive Szenarien ab

**FÃ¼r kritische Features pflegen:**
- Sub-Routen-Generator
- OSRM-Routing
- Tour-Upload & Parsing
- Adress-Matching

**Beispiel: Golden Test fÃ¼r Sub-Routen-Generator**

```python
# tests/golden/test_golden_subroutes.py

GOLDEN_TOUR_W01 = {
    "tour_name": "W01",
    "customers": ["Kunde A", "Kunde B", "Kunde C", "Kunde D", "Kunde E"],
    "expected_subroutes": 2,
    "expected_customer_split": {
        "sub_route_1": ["Kunde A", "Kunde B", "Kunde C"],
        "sub_route_2": ["Kunde D", "Kunde E"]
    }
}

def test_golden_w01_subroutes():
    """Golden Test: W01 muss immer identisch aufgeteilt werden"""
    result = generate_subroutes(GOLDEN_TOUR_W01)
    
    assert len(result["sub_routes"]) == GOLDEN_TOUR_W01["expected_subroutes"]
    assert result["sub_routes"][0]["customers"] == GOLDEN_TOUR_W01["expected_customer_split"]["sub_route_1"]
    assert result["sub_routes"][1]["customers"] == GOLDEN_TOUR_W01["expected_customer_split"]["sub_route_2"]
```

**Pflege:**
- Golden Tests in `tests/golden/` ablegen
- 3-5 Beispieltouren pflegen (W-Touren, groÃŸe Touren, Edge Cases)
- Bei jedem Fix dokumentieren: Welche Golden Tests sind betroffen?
- Bei jedem Fix dokumentieren: Wie manuell prÃ¼fen (UI + Logs)?

**Cursor-Pflicht bei kritischen Fixes:**

```
OUTPUT MUSS ENTHALTEN:

1. Golden Tests, die betroffen sind
   - z.B. "test_golden_w01_subroutes"

2. Manuelle Testanleitung:
   - UI: "Sub-Routen Generator" Button klicken, W01 hochladen
   - Logs: "sub_routes" in Response prÃ¼fen
   - Erwartetes Ergebnis: 2 Sub-Routen, Kunden A-C in Route 1, D-E in Route 2
```

---

## 5. Dokumentation aktualisieren

**Nach jedem relevanten Fix:**

1. **LESSONS_LOG.md:** Neuer Eintrag fÃ¼r wiederkehrende Fehlertypen
2. **API-Dokumentation:** Bei geÃ¤nderten Endpoints aktualisieren
3. **Inline-Kommentare:** Komplexe Fixes kommentieren
4. **CHANGELOG.md:** Nutzer-relevante Ã„nderungen dokumentieren

**Format fÃ¼r LESSONS_LOG.md:**

```md
## YYYY-MM-DD â€“ [Kurzbeschreibung]

**Symptom:** ...
**Ursache:** ...
**Fix:** ...
**Was die KI kÃ¼nftig tun soll:** ...
```

---

## 6. Sicherheit und Robustheit im Blick behalten

### Input-Validierung

- **Backend:** Pydantic-Modelle fÃ¼r alle Requests
- **Frontend:** Defensive Checks vor API-Calls
- **SQL:** Keine String-Konkatenation, immer Prepared Statements

### Fehlerbehandlung

- **Try-Catch:** Alle externen Aufrufe (OSRM, LLM, DB)
- **Logging:** Strukturiertes Logging mit Kontext (keine sensiblen Daten!)
- **User-Feedback:** Klare Fehlermeldungen im UI

### Timeouts

- **OSRM:** Max. 30 Sekunden
- **LLM-APIs:** Max. 60 Sekunden
- **DB-Queries:** Max. 10 Sekunden (Warnung bei Ãœberschreitung)

### Sensitive Daten

**NIEMALS in Logs schreiben:**

- PasswÃ¶rter
- API-Keys
- VollstÃ¤ndige Kundenadressen (nur KÃ¼rzel)
- PersÃ¶nliche Daten (DSGVO)

**Erlaubt:**

- Request-IDs
- Fehler-Codes
- Anonymisierte Daten (z.B. "Kunde #123")

---

## 7. Cursor muss seine Ã„nderungen transparent machen

**Jede Code-Ã„nderung erfordert:**

1. **ErklÃ¤rung:** Warum wurde diese Ã„nderung vorgenommen?
2. **Kontext:** Was wurde behoben / verbessert?
3. **Diff:** Vorher/Nachher klar darstellen
4. **Impact:** Welche anderen Teile sind betroffen?

**Dokumentations-Format:**

```md
### Fix: [Kurzbeschreibung]

**Datei:** `path/to/file.py`
**Zeilen:** 123-145

**Problem:**
- [Was war kaputt]

**LÃ¶sung:**
- [Was wurde geÃ¤ndert]

**Vorher:**
```python
def broken_function():
    return None  # Bug
```

**Nachher:**
```python
def fixed_function():
    return {"data": []}  # Korrekter RÃ¼ckgabewert
```

**Erwartete Userwirkung:**
- [Was sieht/erlebt der Benutzer nach dem Fix?]
```

---

## 8. Audit-Workflow (Schritt fÃ¼r Schritt)

### Phase 1: Vorbereitung

1. **Scope definieren** (siehe Regel 1)

2. **âš ï¸ PFLICHT: Multi-Layer-Kontext sicherstellen**
   
   **Jeder Audit MUSS mindestens eine Datei aus jedem betroffenen Layer im Kontext haben:**
   
   - [ ] **Backend:** Min. 1x `routes/*.py` oder `backend/routes/*.py` oder `services/*.py`
   - [ ] **Frontend:** Min. 1x `frontend/*.js` oder `frontend/*.html`
   - [ ] **Datenbank (falls beteiligt):** `db/schema.py` oder SQL-Dateien
   - [ ] **Infrastruktur (falls beteiligt):** `services/osrm_client.py`, ENV-Config
   
   **Faustregel:** Bug im UI sichtbar = Backend + Frontend PFLICHT!
   
   **Warum?** Verhindert isolierte Fixes, die andere Layer kaputt machen.

3. **Relevante Dateien identifizieren** (Backend, Frontend, DB)
4. **Logs sammeln** (Server-Logs, Browser-Konsole)
5. **Screenshots anfertigen** (falls UI-Bug)

### Phase 2: Analyse

5. **Backend prÃ¼fen** (siehe Regel 2.1)
6. **Frontend prÃ¼fen** (siehe Regel 2.2)
7. **Datenbank prÃ¼fen** (siehe Regel 2.3)
8. **Infrastruktur prÃ¼fen** (siehe Regel 2.4)
9. **API-Kontrakt validieren** (Request/Response)

### Phase 3: Diagnose

10. **Root Cause identifizieren** (nicht nur Symptom!)
11. **Seiteneffekte analysieren** (siehe Regel 3)
12. **Fix-Strategie planen** (Reihenfolge, PrioritÃ¤ten)

### Phase 4: Umsetzung

13. **Code Ã¤ndern** (Backend, Frontend, DB)
14. **Tests schreiben** (siehe Regel 4)
15. **Dokumentation aktualisieren** (siehe Regel 5)
16. **Ã„nderungen erklÃ¤ren** (siehe Regel 7)

### Phase 5: Verifikation

17. **Syntax-Check** (`python -m compileall`, `npm run lint`)
18. **Tests ausfÃ¼hren** (`pytest`, `npm test`)
19. **Manuelle Tests** (UI durchklicken, API-Calls testen)
20. **Logs prÃ¼fen** (keine neuen Fehler)

### Phase 6: Abschluss

21. **Audit-Dokument erstellen** (siehe Regel 9)
22. **ZIP-Archiv anlegen** (siehe Regel 10)
23. **LESSONS_LOG aktualisieren** (falls neuer Fehlertyp)

---

## 9. Audit-Dokumentation (Pflicht-Format)

Jedes Audit erzeugt ein Markdown-Dokument mit folgender Struktur:

```md
# Code-Audit: [Titel]
**Datum:** YYYY-MM-DD
**Bereich:** Backend/Frontend/DB/Infrastruktur
**Dateien:** [Liste]

---

## Executive Summary
âœ… [Anzahl] Fehler behoben
âš ï¸ [Anzahl] Warnungen
ğŸ“Š Code-QualitÃ¤t: [Vorher] â†’ [Nachher]

---

## 1. Problem-Identifikation
### Symptome
### Root Cause

## 2. DurchgefÃ¼hrte Fixes
### Fix 1: [Titel]
### Fix 2: [Titel]
...

## 3. API-Kontrakt-PrÃ¼fung
### Backend-Response
### Frontend-Verarbeitung

## 4. Tests & Verifikation
### Syntax-Check
### Manuelle Tests

## 5. Code-QualitÃ¤t Metriken
### Vorher
### Nachher

## 6. Lessons Learned

## 7. NÃ¤chste Schritte

## 8. Anhang: GeÃ¤nderte Dateien

## 9. Checkliste (abgehakt)
```

---

## 10. ZIP-Archiv-Struktur

Jedes Audit erzeugt ein ZIP im Ordner `zip/` mit folgender Struktur:

```
AUDIT_<THEMA>_YYYYMMDD_HHMMSS_<SESSION_ID>.zip
â”œâ”€â”€ AUDIT_REPORT.md          â† Haupt-Dokument (siehe Regel 9)
â”œâ”€â”€ logs/
â”‚   â”œâ”€â”€ server.log           â† Backend-Logs
â”‚   â”œâ”€â”€ browser-console.txt  â† Frontend-Fehler
â”‚   â””â”€â”€ stacktraces.txt      â† Python-Tracebacks
â”œâ”€â”€ code/
â”‚   â”œâ”€â”€ before/              â† Code VOR dem Fix
â”‚   â”‚   â”œâ”€â”€ file1.py
â”‚   â”‚   â””â”€â”€ file2.js
â”‚   â””â”€â”€ after/               â† Code NACH dem Fix
â”‚       â”œâ”€â”€ file1.py
â”‚       â””â”€â”€ file2.js
â”œâ”€â”€ screenshots/
â”‚   â”œâ”€â”€ error-ui.png
â”‚   â””â”€â”€ fixed-ui.png
â””â”€â”€ tests/
    â””â”€â”€ regression_test.py   â† Neuer Test
```

**Dateinamen-Konvention:**

- `<THEMA>`: Kurzbeschreibung (z.B. `SubRoutenGenerator`, `OSRMTimeout`)
- `YYYYMMDD`: Datum (z.B. `20251114`)
- `HHMMSS`: Uhrzeit (z.B. `143022`)
- `<SESSION_ID>`: Eindeutige ID (optional, z.B. `abc123`)

**Beispiel:**

```
AUDIT_SubRoutenGenerator_20251114_143022_xyz.zip
```

---

## 11. Verbotene Praktiken

**NIEMALS:**

1. âŒ Nur Symptom beheben, Root Cause ignorieren
2. âŒ Code Ã¤ndern, ohne zu testen
3. âŒ Breaking Changes ohne Dokumentation
4. âŒ Sensible Daten in Logs schreiben
5. âŒ Fehler stillschweigend verschlucken (`pass`, leere `except`)
6. âŒ Architektur ohne RÃ¼cksprache umbauen
7. âŒ Alte Bugs mit neuen Bugs Ã¼berdecken
8. âŒ Nicht reproduzierbare Fixes ("hat bei mir funktioniert")

---

## 12. Erlaubte Praktiken

**IMMER:**

1. âœ… Defensive Programmierung (Null-Checks, Type-Checks)
2. âœ… Strukturiertes Logging mit Kontext
3. âœ… Input-Validierung auf allen Ebenen
4. âœ… Fehlerbehandlung mit User-Feedback
5. âœ… Tests fÃ¼r jeden Fix
6. âœ… Klare Commit-Messages / Dokumentation
7. âœ… Code-Reviews (bei kritischen Ã„nderungen)
8. âœ… Performance-Messungen (bei Optimierungen)

---

## 13. Eskalation bei Unsicherheit

**Wenn Cursor sich unsicher ist:**

1. **Dokumentieren:** Was ist unklar? Welche Optionen gibt es?
2. **Fragen:** Explizit nach KlÃ¤rung fragen, bevor Code geÃ¤ndert wird
3. **Alternativen:** Mehrere LÃ¶sungsansÃ¤tze vorschlagen
4. **Risiken:** Potenzielle Seiteneffekte benennen

**Beispiel:**

```md
## Unsicherheit bei Fix-Strategie

**Problem:** API-Response-Format Ã¤ndern (camelCase â†’ snake_case)

**Option 1:** Nur Backend Ã¤ndern
- âœ… Einfach
- âŒ Bricht Frontend

**Option 2:** Backend + Frontend Ã¤ndern
- âœ… Konsistent
- âŒ AufwÃ¤ndiger

**Option 3:** Beide Formate unterstÃ¼tzen (Deprecation)
- âœ… AbwÃ¤rtskompatibel
- âŒ Komplexer

**Empfehlung:** Option 2 (Breaking Change ist akzeptabel in Dev-Phase)
```

---

## 14. Versionierung

Dieses Dokument wird fortlaufend aktualisiert. Ã„nderungen werden dokumentiert:

| Version | Datum | Ã„nderung |
|---------|-------|----------|
| 1.0 | 2025-11-14 | Initiale Version |

---

**Ende der Grundregeln**  
**NÃ¤chste Schritte:** `AUDIT_CHECKLISTE.md` und `LESSONS_LOG.md` lesen

