# Entwicklungsstandards & Richtlinien

**Version:** 2.0 ‚≠ê **KI-Audit-Framework integriert**  
**G√ºltig f√ºr:** Alle FAMO-Projekte  
**Letzte Aktualisierung:** 2025-11-14

---

## üìã Inhaltsverzeichnis

1. [Cursor KI Arbeitsrichtlinien](#cursor-ki-arbeitsrichtlinien)
2. [KI-Audit-Framework (PFLICHT)](#ki-audit-framework-pflicht) ‚≠ê **NEU**
3. [LLM-Integration im Programm](#llm-integration-im-programm) ‚≠ê **NEU**
4. [Coding Standards](#coding-standards)
5. [Architektur-Prinzipien](#architektur-prinzipien)
6. [API-Standards](#api-standards)
7. [Testing-Standards](#testing-standards)
8. [Git & Versionierung](#git--versionierung)
9. [Deployment & Operations](#deployment--operations)
10. [Audit & Compliance](#audit--compliance)
11. [Dokumentations-Standards](#dokumentations-standards)

---

## Cursor KI Arbeitsrichtlinien

### Grundprinzipien

1. **Commit early, commit often**
   - Jeder funktionierende Zwischenstand wird sofort versioniert
   - Stabiler Kontext f√ºr Cursor erhalten
   - Empfohlen: `git commit -m "Checkpoint: Modul X funktionsf√§hig"`

2. **Eine Aufgabe pro Prompt**
   - Pro Prompt nur **eine** Aufgabe
   - ‚ùå "Erstelle Logging, refactore DB und verbessere Auth"
   - ‚úÖ "Erstelle Logging-Service mit File- und Console-Ausgabe"

3. **KI-Vorschl√§ge sind Vorschl√§ge, keine Wahrheit**
   - Vorschl√§ge als Diff pr√ºfen, nicht blind √ºbernehmen
   - Import- und Typfehler entstehen oft durch Autovervollst√§ndigung

### Kontextmanagement

- **Kontext bewusst ausw√§hlen**: Nur relevante Dateien pinnen oder im Prompt benennen
- **Offene Tabs minimieren**: Zu viele offene Dateien f√ºhren zu veralteten Abh√§ngigkeiten
- **Modular arbeiten**: Klare Schnittstellen definieren (TypeScript: `export interface`, Python: `TypedDict`/`Protocol`)

### Abh√§ngigkeiten & Build-Konsistenz

- **Lockfiles nie manuell l√∂schen**: Cursor bezieht API- und Typinformationen daraus
- **Lokaler Build ist ma√ügeblich**: Cursor validiert nur Syntax, nicht Laufzeit
- **Keine Silent-Renames**: Nach jedem gr√∂√üeren KI-Commit `git diff` pr√ºfen

### Versionskontrolle

- **Commit vor jedem KI-Refactor**: Versehentlich zerst√∂rte Module leicht zur√ºckrollen
- **Commit-Messages mit Kontext**: `Refactor: Cursor Vorschlag zu AuthService angewendet`
- **Branching-Strategie**: Cursor-Experimente in eigenen Branches (`feature/ki-login-refactor`)

### Troubleshooting

Wenn nach einer KI-Aktion etwas "nicht mehr geht":
1. `git diff` pr√ºfen ‚Äì oft sind Barrel-Exports oder Pfade ver√§ndert
2. Lokalen Build laufen lassen
3. Cursor-Cache l√∂schen (Command Palette ‚Üí "Clear Editor Context")
4. Bei wiederkehrenden Fehlern: Datei explizit ausschlie√üen (`# KI nicht √§ndern` Kommentar)

---

## KI-Audit-Framework (PFLICHT) ‚≠ê

### üéØ Grundprinzip: Ganzheitliches Denken

**IMMER pr√ºfen:** Backend **UND** Frontend **UND** Datenbank **UND** Infrastruktur

> "Kein isolierter Fix mehr! Jede √Ñnderung wird im Gesamtkontext bewertet."

### Zentrale Dokumentation

Alle KI-Audit-Regeln befinden sich in: **`docs/ki/`**

| Dokument | Zweck | Verbindlich |
|----------|-------|-------------|
| **[README.md](ki/README.md)** | Framework-√úbersicht & Workflow | ‚úÖ JA |
| **[REGELN_AUDITS.md](ki/REGELN_AUDITS.md)** | Grundregeln f√ºr alle Audits | ‚úÖ JA |
| **[AUDIT_CHECKLISTE.md](ki/AUDIT_CHECKLISTE.md)** | 9-Punkte-Checkliste | ‚úÖ JA |
| **[LESSONS_LOG.md](ki/LESSONS_LOG.md)** | Dokumentierte Fehler & L√∂sungen | ‚úÖ JA |
| **[CURSOR_PROMPT_TEMPLATE.md](ki/CURSOR_PROMPT_TEMPLATE.md)** | 10 fertige Audit-Prompts | ‚úÖ JA |

**Quick-Referenz:** [`KI_AUDIT_FRAMEWORK.md`](../KI_AUDIT_FRAMEWORK.md) (Projekt-Root)

### Die 7 Unverhandelbaren Regeln

#### 1. Scope explizit machen
Zu Beginn jedes Audits:
- Welches Feature/Endpoint/UI-Element?
- Welche Symptome (Fehler, Logs, Screenshots)?
- Reproduktionsschritte dokumentieren

#### 2. Immer ganzheitlich pr√ºfen

**Backend (Python/FastAPI):**
- Routes, Services, Business Logic
- Exception-Handling, Logging
- Input-Validierung, Timeouts
- Konfiguration (ENV-Variablen)

**Frontend (HTML/CSS/JavaScript):**
- Entry Points, Event-Handler
- Alle `fetch()` API-Calls
- Request/Response-Kontrakt mit Backend
- Defensive Programmierung (Null-Checks, Array-Validierung)
- Browser-Konsole auf Fehler pr√ºfen

**Datenbank (SQLite):**
- Schema-Konsistenz (Code vs. DB)
- Migrationen (ALTER TABLE, CREATE INDEX)
- Indizes f√ºr Performance
- Datenkonsistenz (Constraints)

**Infrastruktur:**
- OSRM: Erreichbarkeit, Timeouts, Health-Checks
- LLM-APIs: Verf√ºgbarkeit, Fallback-Strategien
- ENV-Variablen: Vollst√§ndigkeit, Defaults
- Docker: Container-Status, Logs

#### 3. Keine isolierten Fixes

**Vor jeder √Ñnderung pr√ºfen:**
1. **Grep/Search:** Wo wird diese Funktion/API noch verwendet?
2. **Impact-Analyse:** Welche Module sind betroffen?
3. **Kontrakt-Pr√ºfung:** √Ñndert sich ein API-Kontrakt (Request/Response)?
4. **Tests anpassen:** Schlagen existierende Tests fehl?

**Beispiel:**
```python
# ‚ùå FALSCH: Nur Backend √§ndern
# Backend: Response-Format ge√§ndert von camelCase zu snake_case
return {"sub_routes": [...]}  # Vorher: {"subRoutes": [...]}

# ‚úÖ RICHTIG: Backend + Frontend + Tests
# 1. Backend: snake_case
# 2. Frontend: data.sub_routes statt data.subRoutes
# 3. Defensive Check: if (data && Array.isArray(data.sub_routes))
# 4. Tests: Erwartetes Format aktualisieren
```

#### 4. Tests sind Pflicht

**F√ºr jeden Bugfix:**
- Mindestens **1 Regressionstest** schreiben
- Test soll sicherstellen, dass Bug nicht zur√ºckkommt
- Test-Kategorien: Unit, Integration, E2E

**Test-Template:**
```python
def test_bugfix_xyz():
    """
    Regression-Test f√ºr Bug #XYZ:
    [Kurzbeschreibung des Bugs]
    """
    # Arrange: Setup
    payload = { ... }
    
    # Act: Aktion
    response = client.post("/api/endpoint", json=payload)
    
    # Assert: Validierung
    assert response.status_code == 200
    data = response.json()
    assert "expected_field" in data
    assert isinstance(data["expected_field"], expected_type)
```

#### 5. Dokumentation aktualisieren

**Nach jedem relevanten Fix:**
1. **LESSONS_LOG.md:** Eintrag f√ºr neue Fehlertypen
2. **API-Dokumentation:** Bei ge√§nderten Endpoints
3. **Inline-Kommentare:** Komplexe Fixes erkl√§ren
4. **CHANGELOG.md:** Nutzer-relevante √Ñnderungen

**LESSONS_LOG-Format:**
```md
## YYYY-MM-DD ‚Äì [Kurzbeschreibung]

**Symptom:** [Was wurde beobachtet?]
**Ursache:** [Root Cause]
**Fix:** [Konkrete L√∂sung]
**Was die KI k√ºnftig tun soll:** [Lehren f√ºr Zukunft]
```

#### 6. Sicherheit und Robustheit

**Input-Validierung:**
- Backend: Pydantic-Modelle f√ºr alle Requests
- Frontend: Defensive Checks vor API-Calls
- SQL: Keine String-Konkatenation, nur Prepared Statements

**Fehlerbehandlung:**
- Try-Catch um externe Aufrufe (OSRM, LLM, DB)
- Strukturiertes Logging mit Kontext
- User-Feedback: Klare Fehlermeldungen im UI

**Timeouts:**
- OSRM: Max. 30 Sekunden
- LLM-APIs: Max. 60 Sekunden
- DB-Queries: Max. 10 Sekunden

**NIEMALS in Logs schreiben:**
- Passw√∂rter, API-Keys
- Vollst√§ndige Kundenadressen
- Pers√∂nliche Daten (DSGVO)

#### 7. Transparenz bei √Ñnderungen

**Jede Code-√Ñnderung erfordert:**
1. **Erkl√§rung:** Warum?
2. **Kontext:** Was wurde behoben?
3. **Diff:** Vorher/Nachher
4. **Impact:** Welche Teile sind betroffen?
5. **Erwartete Userwirkung:** Was √§ndert sich f√ºr den Benutzer?

### Audit-Workflow (6 Phasen)

#### Phase 1: Vorbereitung
1. Scope definieren
2. Relevante Dateien identifizieren
3. Logs sammeln
4. Screenshots anfertigen

#### Phase 2: Analyse
5. Backend pr√ºfen
6. Frontend pr√ºfen
7. Datenbank pr√ºfen
8. Infrastruktur pr√ºfen
9. API-Kontrakt validieren

#### Phase 3: Diagnose
10. Root Cause identifizieren
11. Seiteneffekte analysieren
12. Fix-Strategie planen

#### Phase 4: Umsetzung
13. Code √§ndern
14. Tests schreiben
15. Dokumentation aktualisieren
16. √Ñnderungen erkl√§ren

#### Phase 5: Verifikation
17. Syntax-Check
18. Tests ausf√ºhren
19. Manuelle Tests
20. Logs pr√ºfen

#### Phase 6: Abschluss
21. Audit-Dokument erstellen
22. ZIP-Archiv anlegen (bei gr√∂√üeren Audits)
23. LESSONS_LOG aktualisieren

### Code-Review Standards

**Jedes Code-Review muss:**

‚úÖ **Backend pr√ºfen:**
- Routes, Services, Error-Handling
- Logging, Timeouts, Validierung

‚úÖ **Frontend pr√ºfen:**
- API-Calls, Event-Handler
- Defensive Checks, Error-Boundaries
- Browser-Konsole (keine Fehler)

‚úÖ **API-Kontrakt pr√ºfen:**
- Request/Response-Format konsistent?
- Feldnamen identisch (Backend ‚Üî Frontend)?
- Datentypen kompatibel?

‚úÖ **Datenbank pr√ºfen:**
- Schema-Konsistenz
- Migrationen bei Schema-√Ñnderungen
- Indizes f√ºr Performance

‚úÖ **Tests pr√ºfen:**
- Mindestens 1 Regressionstest
- Coverage ‚â• 80%
- Edge Cases abgedeckt

‚úÖ **Dokumentation pr√ºfen:**
- Code-Kommentare aktualisiert
- LESSONS_LOG bei neuem Fehlertyp
- API-Docs bei Endpoint-√Ñnderungen

### Verbotene Praktiken

**NIEMALS:**

1. ‚ùå Nur Symptom beheben, Root Cause ignorieren
2. ‚ùå Code √§ndern ohne zu testen
3. ‚ùå Breaking Changes ohne Dokumentation
4. ‚ùå Isolierte Fixes (nur Backend ODER nur Frontend)
5. ‚ùå Fehler stillschweigend verschlucken
6. ‚ùå Sensible Daten in Logs
7. ‚ùå Architektur ohne R√ºcksprache umbauen
8. ‚ùå Nicht reproduzierbare Fixes

### Erlaubte Praktiken

**IMMER:**

1. ‚úÖ Ganzheitlich pr√ºfen (Backend + Frontend + DB + Infra)
2. ‚úÖ Defensive Programmierung (Null-Checks, Type-Checks)
3. ‚úÖ Strukturiertes Logging mit Kontext
4. ‚úÖ Input-Validierung auf allen Ebenen
5. ‚úÖ Fehlerbehandlung mit User-Feedback
6. ‚úÖ Tests f√ºr jeden Fix
7. ‚úÖ Klare Commit-Messages
8. ‚úÖ Root Cause identifizieren

### Standard-Prompts f√ºr Cursor

**F√ºr vollst√§ndiges Audit:**
```
F√ºhre einen vollst√§ndigen Code-Audit durch f√ºr: [FEATURE/BUG]

Folge strikt:
- docs/ki/REGELN_AUDITS.md
- docs/ki/AUDIT_CHECKLISTE.md

Pr√ºfe ganzheitlich:
- Backend (Python/FastAPI)
- Frontend (HTML/CSS/JavaScript)
- Datenbank (SQLite)
- Infrastruktur (OSRM, ENV)

Identifiziere:
- Root Cause (nicht nur Symptom!)
- API-Kontrakte (Backend ‚Üî Frontend)
- Seiteneffekte

Liefere:
- Konkrete Fixes (mit Dateinamen und Zeilen)
- Mindestens 1 Regressionstest
- Audit-Dokument nach docs/ki/REGELN_AUDITS.md
- LESSONS_LOG-Eintrag (falls neuer Fehlertyp)
```

**Weitere Prompts:** Siehe `docs/ki/CURSOR_PROMPT_TEMPLATE.md`

### Audit-Dokumentation

**Jedes Audit erzeugt:**

1. **Audit-Dokument** (Markdown)
   - Executive Summary
   - Problem-Identifikation (Symptom + Root Cause)
   - Durchgef√ºhrte Fixes (Vorher/Nachher)
   - Tests & Verifikation
   - Code-Qualit√§t Metriken
   - Lessons Learned
   - N√§chste Schritte

2. **ZIP-Archiv** (bei gr√∂√üeren Audits)
   - Struktur: `ZIP/AUDIT_<THEMA>_YYYYMMDD_HHMMSS.zip`
   - Enth√§lt: Logs, Code (Vorher/Nachher), Screenshots, Tests

3. **LESSONS_LOG-Eintrag** (bei neuem Fehlertyp)
   - Symptom, Ursache, Fix, Lehren f√ºr Zukunft

### Metriken & Monitoring

**Code-Qualit√§t tracken:**

| Metrik | Ziel |
|--------|------|
| Syntax-Fehler | 0 |
| Defensive Checks | Alle kritischen Pfade |
| Memory Leaks | 0 |
| JSDoc/Docstring Coverage | ‚â• 80% |
| Browser-Kompatibilit√§t | Moderne Browser + Fallbacks |
| Test-Coverage | ‚â• 80% |

**H√§ufigste Fehlertypen dokumentieren:**
- Schema-Drift (DB)
- Syntax-Fehler (Frontend/Backend)
- Missing Defensive Checks
- Memory Leaks
- API-Kontrakt-Inkonsistenzen

### Eskalation

**Bei Unsicherheit:**

1. **Dokumentieren:** Was ist unklar? Welche Optionen?
2. **Fragen:** Explizit nach Kl√§rung fragen
3. **Alternativen:** Mehrere L√∂sungsans√§tze vorschlagen
4. **Risiken:** Potenzielle Seiteneffekte benennen

**Beispiel:**
```md
## Unsicherheit bei Fix-Strategie

**Problem:** API-Response-Format √§ndern

**Option 1:** Nur Backend √§ndern
- ‚úÖ Einfach
- ‚ùå Bricht Frontend

**Option 2:** Backend + Frontend √§ndern
- ‚úÖ Konsistent
- ‚ùå Aufw√§ndiger

**Empfehlung:** Option 2 (ganzheitlich!)
```

---

## LLM-Integration im Programm ‚≠ê

### ü§ñ √úbersicht: LLM im Produktions-Code

**Zweck:** Regeln f√ºr die Integration von Large Language Models (OpenAI/Ollama) in der FAMO TrafficApp

**Anwendungsf√§lle:**
- Adress-Erkennung (aus unstrukturierten Texten)
- Geocoding-Verbesserung
- Kunden-Matching
- Tour-Klassifizierung

**Wichtig:** LLM ist **Werkzeug**, keine Magie! Strikte Regeln befolgen!

---

### üéØ Grundprinzip: LLM nur als letzter Fallback

**Defense-in-Depth Strategie:**

```
1. Blacklist      ‚Üê Bekannt fehlerhafte Adressen
2. Exact Match    ‚Üê Exakte √úbereinstimmungen  
3. Regex          ‚Üê Musterbasierte Erkennung
4. Gazetteer      ‚Üê Ortsverzeichnis
5. Postal         ‚Üê Postleitzahlen-DB
6. Rules          ‚Üê Regelbasierte Logik
7. LLM (Fallback) ‚Üê NUR wenn alles andere fehlschl√§gt!
```

**Regel:** LLM ist **nicht** die erste Wahl, sondern die **letzte**!

**Warum?**
- Deterministisch > Probabilistisch
- Schnell > Langsam
- Kostenlos > Kostenpflichtig
- Nachvollziehbar > Black Box

---

### ‚õî Die 10 Verbote f√ºr LLM-Nutzung

#### 1. Kein LLM ohne Schema-Validierung

‚ùå **FALSCH:**
```python
result = llm.generate(prompt)
return result  # Blind vertrauen!
```

‚úÖ **RICHTIG:**
```python
result = llm.generate(prompt)
validated = AddressSchema.parse(result)  # Pydantic-Validierung!
if not validated:
    raise ValidationError("LLM-Schema ung√ºltig")
return validated
```

#### 2. Kein LLM ohne Verifikation

‚ùå **FALSCH:**
```python
address = llm.extract_address(text)
save_to_db(address)  # Keine Pr√ºfung!
```

‚úÖ **RICHTIG:**
```python
address = llm.extract_address(text)
if not is_plausible_address(address):
    log_to_quarantine(address, reason="LLM-Validation failed")
    metrics.increment("llm_invalid_result")
    raise ValidationError("Ung√ºltige Adresse")
save_to_db(address)
```

#### 3. Kein LLM ohne Timeout

‚ùå **FALSCH:**
```python
response = llm_client.generate(prompt)  # Kann ewig dauern
```

‚úÖ **RICHTIG:**
```python
response = llm_client.generate(
    prompt, 
    timeout=60  # Max. 60 Sekunden
)
```

#### 4. Kein LLM ohne Fehlerbehandlung

‚ùå **FALSCH:**
```python
try:
    result = llm.process(data)
except:
    result = None  # Silent fail
```

‚úÖ **RICHTIG:**
```python
try:
    result = llm.process(data)
except LLMTimeout as e:
    logger.error(f"LLM Timeout: {e}", extra={"correlation_id": "..."})
    metrics.increment("llm_timeout")
    # Fallback auf regelbasierte Logik
    result = fallback_parser(data)
except LLMInvalidSchema as e:
    logger.error(f"LLM Invalid Schema: {e}")
    metrics.increment("llm_invalid_schema")
    result = fallback_parser(data)
```

#### 5. Kein LLM ohne Monitoring

‚úÖ **PFLICHT: Metriken tracken**
```python
metrics.increment("llm_success")
metrics.increment("llm_failure")
metrics.increment("llm_timeout")
metrics.increment("llm_invalid_schema")
metrics.histogram("llm_latency_ms", value=latency)
metrics.gauge("llm_tokens_used", tokens)
metrics.gauge("llm_cost_usd", cost)
```

#### 6. Kein LLM ohne Rate-Limiting

‚úÖ **PFLICHT: Rate-Limits setzen**
```python
if llm_rate_limiter.is_exceeded():
    logger.warning("LLM Rate-Limit erreicht")
    metrics.increment("llm_rate_limited")
    return fallback_parser(data)
```

#### 7. Kein LLM ohne Kosten-Kontrolle

‚úÖ **PFLICHT: Budget √ºberwachen**
```python
daily_cost = metrics.get("llm_cost_usd_today")
if daily_cost > DAILY_BUDGET:
    logger.error(f"LLM Budget √ºberschritten: ${daily_cost}")
    alert("LLM-Kosten zu hoch!")
    # Deaktiviere LLM tempor√§r
    ENABLE_LLM = False
```

#### 8. Kein LLM ohne Fallback

‚úÖ **PFLICHT: Regelbasierter Fallback**
```python
def process_with_fallback(data):
    # 1. Versuche LLM
    if ENABLE_LLM:
        try:
            return llm_parser(data)
        except LLMError:
            pass
    
    # 2. Fallback: Regelbasiert
    return rule_based_parser(data)
```

#### 9. Kein LLM ohne Determinismus

‚úÖ **PFLICHT: Temperature = 0.0**
```python
# Konfiguration
OPENAI_TEMPERATURE = 0.0  # Deterministisch!
OPENAI_SEED = 42          # Reproduzierbar

# Bei jedem Call
response = llm.generate(
    prompt=prompt,
    temperature=0.0,  # Gleicher Input = Gleicher Output
    seed=42
)
```

#### 10. Kein LLM ohne PII-Schutz

‚ùå **NIEMALS loggen:**
```python
logger.info(f"LLM Response: {full_customer_address}")  # PII!
logger.info(f"API Key: {OPENAI_API_KEY}")  # Secret!
```

‚úÖ **RICHTIG:**
```python
logger.info("LLM erfolgreich", extra={
    "customer_id": "K123",  # Nur ID
    "plz": "01234",         # OK
    "city": "Dresden",      # OK  
    "street": "***",        # Anonymisiert!
    "latency_ms": 450
})
```

---

### üîß Sichere LLM-Integration (Template)

```python
from pydantic import BaseModel, ValidationError
from typing import Optional
import logging

logger = logging.getLogger(__name__)

class AddressSchema(BaseModel):
    """Validiertes Schema f√ºr LLM-Antworten"""
    street: str
    number: str
    plz: str
    city: str
    confidence: float

def use_llm_safely(prompt: str, data: dict) -> Optional[AddressSchema]:
    """
    Sichere LLM-Nutzung mit allen Pflicht-Checks
    
    Returns:
        AddressSchema bei Erfolg
        None bei Fehler (Fallback nutzen!)
    """
    
    # 1. Pre-Check: Ist LLM verf√ºgbar?
    if not llm_client.is_available():
        logger.warning("LLM nicht verf√ºgbar, nutze Fallback")
        metrics.increment("llm_unavailable")
        return None
    
    # 2. Rate-Limiting pr√ºfen
    if llm_rate_limiter.is_exceeded():
        logger.warning("LLM Rate-Limit erreicht")
        metrics.increment("llm_rate_limited")
        return None
    
    # 3. Budget-Check
    if daily_cost_exceeded():
        logger.error("LLM Budget √ºberschritten")
        metrics.increment("llm_budget_exceeded")
        return None
    
    # 4. LLM-Call mit Timeout
    start_time = time.time()
    try:
        response = llm_client.generate(
            prompt=prompt,
            timeout=60,
            temperature=0.0,  # Deterministisch!
            max_tokens=500
        )
    except LLMTimeout as e:
        logger.error(f"LLM Timeout: {e}")
        metrics.increment("llm_timeout")
        return None
    except LLMError as e:
        logger.error(f"LLM Error: {e}")
        metrics.increment("llm_error")
        return None
    
    latency = (time.time() - start_time) * 1000
    
    # 5. Schema-Validierung (Pydantic)
    try:
        validated = AddressSchema.parse_raw(response)
    except ValidationError as e:
        logger.error(f"LLM Schema-Validierung fehlgeschlagen: {e}")
        metrics.increment("llm_invalid_schema")
        return None
    
    # 6. Business-Logic-Validierung
    if not is_plausible_address(validated):
        logger.warning("LLM-Adresse nicht plausibel")
        metrics.increment("llm_implausible")
        return None
    
    # 7. Confidence-Check
    if validated.confidence < LLM_CONFIDENCE_THRESHOLD:
        logger.warning(f"LLM Confidence zu niedrig: {validated.confidence}")
        metrics.increment("llm_low_confidence")
        return None
    
    # 8. Logging & Metriken (ohne PII!)
    logger.info("LLM erfolgreich", extra={
        "latency_ms": latency,
        "tokens": response.tokens,
        "confidence": validated.confidence,
        "plz": validated.plz,  # OK
        "city": validated.city  # OK
        # KEIN street, KEIN number!
    })
    metrics.increment("llm_success")
    metrics.histogram("llm_latency_ms", latency)
    metrics.gauge("llm_tokens", response.tokens)
    
    # 9. Kosten tracken
    cost = calculate_cost(response.tokens)
    metrics.gauge("llm_cost_usd", cost)
    
    return validated
```

---

### ‚öôÔ∏è Konfiguration (ENV-Variablen)

**OpenAI:**
```bash
OPENAI_API_KEY=sk-...           # API-Key (secret!)
OPENAI_MODEL=gpt-4-turbo        # Modell
OPENAI_TIMEOUT=60               # Max. 60 Sekunden
OPENAI_MAX_TOKENS=500           # Token-Limit
OPENAI_TEMPERATURE=0.0          # Deterministisch!
OPENAI_SEED=42                  # Reproduzierbar
```

**Ollama (lokal):**
```bash
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama2
OLLAMA_TIMEOUT=60
```

**Feature-Flags:**
```bash
ENABLE_LLM_FALLBACK=true        # LLM als Fallback aktivieren?
LLM_CONFIDENCE_THRESHOLD=0.8    # Min. Confidence (0.0-1.0)
LLM_DAILY_BUDGET_USD=50         # Max. Kosten pro Tag
LLM_RATE_LIMIT=100              # Max. Calls pro Minute
```

---

### üìä Monitoring & Alarme

**Metriken (Pflicht!):**

| Metrik | Zweck | Alarm bei |
|--------|-------|-----------|
| `llm_success` | Erfolgreiche Calls | - |
| `llm_failure` | Fehlgeschlagene Calls | > 10% aller Calls |
| `llm_timeout` | Timeouts | > 5 in 5min |
| `llm_invalid_schema` | Schema-Fehler | > 0 |
| `llm_latency_ms` | Response-Zeit | > 10s |
| `llm_tokens` | Token-Verbrauch | - |
| `llm_cost_usd` | Kosten (OpenAI) | > Budget |

**Alarme setzen:**
```python
# ‚ö†Ô∏è Schema-Fehler ‚Üí Sofort Review!
if llm_invalid_schema > 0:
    alert("LLM Schema-Validation fehlgeschlagen ‚Üí Prompt pr√ºfen!")

# ‚ö†Ô∏è Viele Timeouts ‚Üí Service down?
if llm_timeout > 5 in 5min:
    alert("Viele LLM Timeouts ‚Üí OpenAI/Ollama down?")

# ‚ö†Ô∏è Hohe Kosten
if llm_cost_today > DAILY_BUDGET:
    alert(f"LLM Budget √ºberschritten: ${llm_cost_today}")
    
# ‚ö†Ô∏è Niedrige Erfolgsrate
success_rate = llm_success / (llm_success + llm_failure)
if success_rate < 0.8:
    alert(f"LLM Erfolgsrate niedrig: {success_rate:.0%}")
```

---

### üß™ Testing

**Unit Tests (mit Mocks):**
```python
def test_llm_with_mock():
    """LLM-Integration mit Mock testen (kein echtes LLM)"""
    
    mock_response = {
        "street": "Hauptstra√üe",
        "number": "123",
        "plz": "01234",
        "city": "Dresden",
        "confidence": 0.95
    }
    
    with patch('llm_client.generate', return_value=mock_response):
        result = use_llm_safely(prompt="...", data={})
        
        assert result.street == "Hauptstra√üe"
        assert result.confidence == 0.95

def test_llm_timeout_fallback():
    """LLM-Timeout ‚Üí Fallback testen"""
    
    with patch('llm_client.generate', side_effect=LLMTimeout):
        result = process_with_fallback(data)
        
        # Sollte regelbasierten Fallback nutzen
        assert result is not None
        assert result.source == "fallback"

def test_llm_invalid_schema():
    """Ung√ºltiges LLM-Schema ‚Üí Exception"""
    
    invalid_response = {"invalid": "data"}
    
    with patch('llm_client.generate', return_value=invalid_response):
        result = use_llm_safely(prompt="...", data={})
        
        assert result is None  # Schema-Validierung schl√§gt fehl
```

**Golden Tests:**
```python
def test_llm_with_known_addresses():
    """Test mit bekannten Problem-Adressen"""
    
    golden_cases = [
        ("Hauptstr. 1, 01234 Dresden", "Hauptstra√üe", "1", "01234", "Dresden"),
        ("Am Markt 5, Heidenau", "Am Markt", "5", None, "Heidenau"),
        # ... mehr F√§lle
    ]
    
    for input_text, expected_street, expected_nr, expected_plz, expected_city in golden_cases:
        result = use_llm_safely(prompt=input_text, data={})
        
        assert result.street == expected_street
        assert result.number == expected_nr
        # ...
```

---

### üí∞ Kosten-Kontrolle

**OpenAI Kosten berechnen:**
```python
class CostTracker:
    """OpenAI Kosten tracken"""
    
    COSTS = {
        "gpt-4-turbo": {
            "input": 0.01,   # $ pro 1K tokens
            "output": 0.03   # $ pro 1K tokens
        }
    }
    
    def calculate_cost(self, model: str, input_tokens: int, output_tokens: int) -> float:
        """Kosten berechnen"""
        costs = self.COSTS[model]
        input_cost = (input_tokens / 1000) * costs["input"]
        output_cost = (output_tokens / 1000) * costs["output"]
        total = input_cost + output_cost
        
        # Loggen
        logger.info(f"LLM-Kosten: ${total:.4f}", extra={
            "input_tokens": input_tokens,
            "output_tokens": output_tokens
        })
        metrics.gauge("llm_cost_usd", total)
        
        return total
    
    def check_daily_budget(self, cost: float) -> bool:
        """Budget-Check"""
        today = date.today()
        daily_total = metrics.get(f"llm_cost_usd_total_{today}")
        
        if daily_total + cost > DAILY_BUDGET:
            logger.error(f"Budget √ºberschritten: ${daily_total + cost:.2f}")
            return False
        
        return True
```

---

### üìö Dokumentation

**Prompt-Dokumentation:**

Alle LLM-Prompts in `ai_models/pdf_ai_prompt.txt` oder √§hnlich dokumentieren:

```markdown
# LLM-Prompt: Adress-Erkennung

## Zweck
Extrahiere strukturierte Adresse aus unstrukturiertem Text

## Input-Format
Text mit Adress-Informationen (Stra√üe, Nummer, PLZ, Ort)

## Output-Format (JSON)
{
  "street": "Stra√üenname",
  "number": "Hausnummer",
  "plz": "Postleitzahl",
  "city": "Ort",
  "confidence": 0.0-1.0
}

## Beispiele
Input: "Hauptstr. 1, 01234 Dresden"
Output: {"street": "Hauptstra√üe", "number": "1", "plz": "01234", "city": "Dresden", "confidence": 0.95}

## Validierung
- street: nicht leer
- number: numerisch oder alphanumerisch (z.B. "12a")
- plz: 5 Ziffern (Deutschland)
- city: nicht leer
- confidence: 0.0-1.0
```

---

### üîÑ Fallback-Strategie

**Hierarchie:**

```
1. Regelbasierte Erkennung (schnell, deterministisch)
   ‚Üì Fehlschlag
2. Gazetteer/Postal-Lookup (mittel, deterministisch)
   ‚Üì Fehlschlag
3. LLM (langsam, probabilistisch)
   ‚Üì Fehlschlag
4. Quarant√§ne (manuell pr√ºfen)
```

**Implementierung:**
```python
def process_address(text: str) -> Address:
    """
    Versuche mehrere Methoden in Reihenfolge
    """
    
    # 1. Regelbasiert (Regex, Pattern-Matching)
    try:
        address = rule_based_parser(text)
        if address.confidence > 0.9:
            metrics.increment("address_parsed_rules")
            return address
    except ParsingError:
        pass
    
    # 2. Gazetteer/Postal (Ortsverzeichnis, PLZ-DB)
    try:
        address = gazetteer_parser(text)
        if address.confidence > 0.85:
            metrics.increment("address_parsed_gazetteer")
            return address
    except ParsingError:
        pass
    
    # 3. LLM (falls aktiviert)
    if ENABLE_LLM_FALLBACK:
        address = use_llm_safely(prompt=text, data={})
        if address and address.confidence > LLM_CONFIDENCE_THRESHOLD:
            metrics.increment("address_parsed_llm")
            return address
    
    # 4. Quarant√§ne (manuell pr√ºfen)
    logger.error("Adresse konnte nicht geparst werden", extra={
        "text_preview": text[:50] + "...",
        "correlation_id": "..."
    })
    quarantine.add(text, reason="Parsing failed (all methods)")
    metrics.increment("address_quarantine")
    raise ParsingError("Adresse nicht erkennbar")
```

---

### ‚úÖ Checkliste: LLM-Integration

- [ ] **Schema-Validierung** (Pydantic)
- [ ] **Timeout** (60s max)
- [ ] **Fehlerbehandlung** (Try-Catch, Logging)
- [ ] **Monitoring** (Metriken, Alarme)
- [ ] **Rate-Limiting** (Calls pro Minute begrenzen)
- [ ] **Kosten-Kontrolle** (Budget, Alarme)
- [ ] **Fallback** (regelbasiert wenn LLM fehlschl√§gt)
- [ ] **PII-Schutz** (keine sensiblen Daten loggen)
- [ ] **Deterministisch** (Temperature = 0.0)
- [ ] **Tests** (Unit, Golden, Mocks)
- [ ] **Dokumentation** (Prompts, Schema, Beispiele)

---

## LLM f√ºr Code-Analyse & Code-Review ‚≠ê

### üîç √úbersicht: LLM als Code-Analyzer

**Zweck:** Regeln f√ºr die Nutzung von LLMs zur automatischen Code-Analyse und Code-Review

**Anwendungsf√§lle:**
- Automatische Code-Reviews (Qualit√§t, Best Practices)
- Pattern-Erkennung (Anti-Patterns, Code Smells)
- Security-Analyse (Schwachstellen, Secrets)
- Dokumentations-Generierung (Docstrings, README)
- Refactoring-Vorschl√§ge
- Test-Generierung

**Status:** üöß In Entwicklung / Experimentell

---

### üéØ Grundprinzip: LLM als Assistent, nicht als Entscheider

**Regel:** LLM gibt **Vorschl√§ge**, Entwickler **entscheidet**!

```
LLM-Analyse
    ‚Üì
Vorschl√§ge generieren
    ‚Üì
Entwickler reviewt
    ‚Üì
Entwickler akzeptiert/ablehnt
    ‚Üì
√Ñnderungen werden umgesetzt
```

**WICHTIG:** LLM ersetzt NICHT den menschlichen Code-Review!

---

### üìã Anwendungsfall 1: Automatischer Code-Review

**Ziel:** LLM analysiert Code und gibt Feedback zu:
- Code-Qualit√§t (Lesbarkeit, Wartbarkeit)
- Best Practices (PEP 8, FastAPI-Patterns)
- Potenzielle Bugs (Null-Checks, Error-Handling)
- Performance (ineffiziente Schleifen, DB-Queries)

**Implementierung:**

```python
def llm_code_review(file_path: str, code: str) -> CodeReviewResult:
    """
    LLM-basierter Code-Review
    
    Returns:
        CodeReviewResult mit Vorschl√§gen
    """
    
    prompt = f"""
Analysiere folgenden Python-Code und gib strukturiertes Feedback:

Datei: {file_path}

Code:
```python
{code}
```

Pr√ºfe:
1. Code-Qualit√§t (Lesbarkeit, Wartbarkeit)
2. Best Practices (PEP 8, Type-Hints, Docstrings)
3. Potenzielle Bugs (Null-Checks, Error-Handling, Edge Cases)
4. Performance (ineffiziente Operationen, N+1 Queries)
5. Security (SQL-Injection, XSS, Secrets im Code)

Format der Antwort (JSON):
{{
  "severity": "info|warning|error|critical",
  "category": "quality|best_practice|bug|performance|security",
  "line": 42,
  "message": "Kurzbeschreibung",
  "suggestion": "Konkreter Vorschlag zur Behebung",
  "example": "Code-Beispiel (optional)"
}}
"""
    
    # LLM-Call mit Validierung
    try:
        response = llm_client.generate(
            prompt=prompt,
            timeout=30,
            temperature=0.1,  # Leicht kreativ f√ºr Vorschl√§ge
            max_tokens=2000
        )
        
        # Schema-Validierung
        results = CodeReviewSchema.parse(response)
        
        # Filtern: Nur relevante Vorschl√§ge
        filtered = [r for r in results if r.severity in ['warning', 'error', 'critical']]
        
        return CodeReviewResult(
            file=file_path,
            suggestions=filtered,
            llm_used=True
        )
        
    except (LLMTimeout, ValidationError) as e:
        logger.warning(f"LLM Code-Review fehlgeschlagen: {e}")
        return CodeReviewResult(
            file=file_path,
            suggestions=[],
            llm_used=False,
            error=str(e)
        )
```

**Output-Schema:**

```python
from pydantic import BaseModel
from typing import Literal

class CodeReviewSuggestion(BaseModel):
    severity: Literal["info", "warning", "error", "critical"]
    category: Literal["quality", "best_practice", "bug", "performance", "security"]
    line: int
    message: str
    suggestion: str
    example: Optional[str] = None

class CodeReviewResult(BaseModel):
    file: str
    suggestions: List[CodeReviewSuggestion]
    llm_used: bool
    error: Optional[str] = None
```

---

### üìã Anwendungsfall 2: Security-Analyse

**Ziel:** LLM findet potenzielle Sicherheitsl√ºcken

**Zu pr√ºfen:**
- SQL-Injection (String-Konkatenation in Queries)
- XSS (Unescaped User-Input)
- Secrets im Code (API-Keys, Passw√∂rter)
- Path-Traversal (User-Input in Dateipfaden)
- Command-Injection (User-Input in Shell-Befehlen)

**Implementierung:**

```python
def llm_security_scan(code: str) -> List[SecurityIssue]:
    """
    LLM-basierte Security-Analyse
    """
    
    prompt = f"""
Analysiere folgenden Code auf Sicherheitsl√ºcken:

```python
{code}
```

Pr√ºfe insbesondere:
1. SQL-Injection (String-Konkatenation statt Prepared Statements)
2. XSS (Unescaped User-Input in Templates)
3. Secrets (API-Keys, Passw√∂rter im Code)
4. Path-Traversal (User-Input in Dateipfaden ohne Validierung)
5. Command-Injection (User-Input in os.system, subprocess)
6. Fehlende Input-Validierung
7. Fehlende Authentication/Authorization

Format (JSON):
{{
  "issue_type": "sql_injection|xss|secrets|path_traversal|command_injection",
  "severity": "low|medium|high|critical",
  "line": 42,
  "description": "Beschreibung des Problems",
  "fix": "Konkrete L√∂sung",
  "cwe": "CWE-89" // Common Weakness Enumeration
}}
"""
    
    response = llm_client.generate(prompt, timeout=30)
    issues = SecurityIssueSchema.parse(response)
    
    # Filtern: Nur High + Critical
    critical_issues = [i for i in issues if i.severity in ['high', 'critical']]
    
    if critical_issues:
        alert(f"Security-Issues gefunden: {len(critical_issues)}")
    
    return critical_issues
```

---

### üìã Anwendungsfall 3: Test-Generierung

**Ziel:** LLM generiert Unit-Tests f√ºr bestehenden Code

**Vorgehen:**

```python
def llm_generate_tests(function_code: str, function_name: str) -> str:
    """
    Generiere Unit-Tests f√ºr eine Funktion
    """
    
    prompt = f"""
Generiere Unit-Tests (pytest) f√ºr folgende Funktion:

```python
{function_code}
```

Anforderungen:
1. Teste Happy Path (normale Verwendung)
2. Teste Edge Cases (leere Inputs, None, etc.)
3. Teste Error-Handling (Exceptions)
4. Nutze pytest-Fixtures wo sinnvoll
5. Nutze Mocks f√ºr externe Dependencies
6. Teste alle Branches (if/else)

Format:
```python
import pytest
from unittest.mock import patch, Mock

def test_{function_name}_happy_path():
    # Arrange
    ...
    # Act
    ...
    # Assert
    ...

def test_{function_name}_edge_case_empty_input():
    ...

def test_{function_name}_error_handling():
    with pytest.raises(ValueError):
        ...
```
"""
    
    response = llm_client.generate(prompt, timeout=60, temperature=0.2)
    
    # Validiere generierten Code (Syntax-Check)
    try:
        compile(response, '<string>', 'exec')
    except SyntaxError as e:
        logger.error(f"LLM generierte ung√ºltigen Code: {e}")
        return None
    
    return response
```

---

### üìã Anwendungsfall 4: Dokumentations-Generierung

**Ziel:** LLM generiert Docstrings und README-Abschnitte

**Implementierung:**

```python
def llm_generate_docstring(function_code: str) -> str:
    """
    Generiere Google-Style Docstring f√ºr Funktion
    """
    
    prompt = f"""
Generiere einen Google-Style Docstring f√ºr folgende Funktion:

```python
{function_code}
```

Format:
```python
def function_name(...):
    \"\"\"Kurzbeschreibung (eine Zeile).
    
    L√§ngere Beschreibung (optional).
    
    Args:
        param1 (type): Beschreibung
        param2 (type): Beschreibung
    
    Returns:
        return_type: Beschreibung
    
    Raises:
        ExceptionType: Wann wird diese Exception geworfen?
    
    Example:
        >>> function_name(param1, param2)
        expected_output
    \"\"\"
```
"""
    
    response = llm_client.generate(prompt, timeout=20)
    return response
```

---

### ‚ö†Ô∏è Einschr√§nkungen & Risiken

**1. Halluzinationen:**
- LLM kann falsche Bugs "finden" (False Positives)
- LLM kann echte Bugs √ºbersehen (False Negatives)
- **L√∂sung:** Immer durch Menschen validieren!

**2. Kontext-Limitierungen:**
- LLM sieht nur einen File, nicht das ganze System
- Kann Abh√§ngigkeiten nicht vollst√§ndig verstehen
- **L√∂sung:** Kontext explizit mitgeben (Imports, Dependencies)

**3. Kosten:**
- Code-Review f√ºr gro√üe Codebase kann teuer werden
- OpenAI: ~$0.01 pro 1K Tokens
- **L√∂sung:** Nur f√ºr neue/ge√§nderte Dateien, nicht ganze Codebase

**4. Datenschutz:**
- Code k√∂nnte sensible Informationen enthalten
- OpenAI speichert Anfragen (30 Tage)
- **L√∂sung:** Lokales LLM (Ollama) f√ºr sensiblen Code

---

### üõ†Ô∏è Best Practices f√ºr LLM-Code-Analyse

#### 1. Kontext mitgeben

```python
# ‚úÖ RICHTIG: Kontext mitgeben
prompt = f"""
Projekt: FAMO TrafficApp
Framework: FastAPI + Pydantic
Datenbank: SQLite

Abh√§ngigkeiten:
- from pydantic import BaseModel
- from fastapi import APIRouter

Code:
{code}

Pr√ºfe auf Best Practices f√ºr FastAPI...
"""

# ‚ùå FALSCH: Kein Kontext
prompt = f"Pr√ºfe diesen Code: {code}"
```

#### 2. Spezifische Pr√ºfungen

```python
# ‚úÖ RICHTIG: Spezifisch
prompt = "Pr√ºfe auf SQL-Injection in DB-Queries"

# ‚ùå FALSCH: Zu allgemein
prompt = "Pr√ºfe auf Fehler"
```

#### 3. Output validieren

```python
# ‚úÖ RICHTIG: Schema-Validierung
results = CodeReviewSchema.parse(llm_response)

# Zus√§tzlich: Plausibilit√§ts-Check
for suggestion in results:
    if suggestion.line > total_lines:
        logger.warning(f"LLM gab ung√ºltige Zeile an: {suggestion.line}")
        continue
```

#### 4. Menschliche Review-Pflicht

```python
# ‚úÖ RICHTIG: LLM + Mensch
llm_suggestions = llm_code_review(code)
human_approved = []

for suggestion in llm_suggestions:
    if suggestion.severity == 'critical':
        # Automatisch akzeptieren
        human_approved.append(suggestion)
    else:
        # Zur manuellen Review
        review_queue.add(suggestion)

# ‚ùå FALSCH: Blind akzeptieren
for suggestion in llm_suggestions:
    apply_fix(suggestion)  # Ohne Review!
```

---

### üìä Metriken f√ºr Code-Analyse

**Tracken:**

```python
# Erfolgsmetriken
metrics.increment("llm_code_review_completed")
metrics.gauge("llm_suggestions_count", len(suggestions))
metrics.gauge("llm_critical_issues", len(critical_issues))

# Akzeptanz-Rate
metrics.increment("llm_suggestions_accepted")
metrics.increment("llm_suggestions_rejected")

# False Positives
metrics.increment("llm_false_positive")  # Manuell markiert

# Kosten
metrics.gauge("llm_code_review_cost_usd", cost)
```

**Auswertung:**

```python
# Akzeptanz-Rate berechnen
acceptance_rate = accepted / (accepted + rejected)

if acceptance_rate < 0.5:
    logger.warning(f"LLM Akzeptanz-Rate niedrig: {acceptance_rate:.0%}")
    # Prompt optimieren!
```

---

### üîÑ Workflow: LLM-Code-Review Integration

**In CI/CD-Pipeline:**

```yaml
# .github/workflows/code-review.yml
name: LLM Code Review

on:
  pull_request:
    types: [opened, synchronize]

jobs:
  llm-review:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      
      - name: Get changed files
        id: changed-files
        uses: tj-actions/changed-files@v35
      
      - name: LLM Code Review
        run: |
          for file in ${{ steps.changed-files.outputs.all_changed_files }}; do
            if [[ $file == *.py ]]; then
              python scripts/llm_code_review.py --file $file
            fi
          done
      
      - name: Post Review Comments
        uses: actions/github-script@v6
        with:
          script: |
            // Poste LLM-Vorschl√§ge als PR-Kommentare
            const suggestions = JSON.parse(fs.readFileSync('review-results.json'));
            for (const suggestion of suggestions) {
              github.rest.pulls.createReviewComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                pull_number: context.issue.number,
                body: `ü§ñ LLM-Vorschlag: ${suggestion.message}\n\n${suggestion.suggestion}`,
                path: suggestion.file,
                line: suggestion.line
              });
            }
```

---

### ‚úÖ Checkliste: LLM-Code-Analyse

- [ ] **Kontext mitgeben** (Projekt, Framework, Dependencies)
- [ ] **Spezifische Pr√ºfungen** (Security, Performance, Best Practices)
- [ ] **Output-Validierung** (Schema, Plausibilit√§t)
- [ ] **Menschliche Review** (Keine automatische Akzeptanz!)
- [ ] **Metriken tracken** (Akzeptanz-Rate, False Positives, Kosten)
- [ ] **False-Positive-Feedback** (LLM-Vorschl√§ge bewerten)
- [ ] **Prompt-Optimierung** (Bei niedriger Akzeptanz-Rate)
- [ ] **Datenschutz** (Lokales LLM f√ºr sensiblen Code)
- [ ] **Kosten-Kontrolle** (Budget f√ºr Code-Reviews)
- [ ] **Integration in CI/CD** (Automatisch bei PRs)

---

### üöß Roadmap: Zuk√ºnftige Erweiterungen

**Phase 1: Proof of Concept (aktuell)**
- ‚úÖ Basis-Code-Review (Qualit√§t, Best Practices)
- ‚úÖ Security-Scan (SQL-Injection, XSS, Secrets)
- ‚úÖ Schema-Validierung

**Phase 2: Erweiterte Analyse**
- [ ] Performance-Analyse (N+1 Queries, ineffiziente Schleifen)
- [ ] Refactoring-Vorschl√§ge (Code Smells, Duplikate)
- [ ] Test-Coverage-Analyse (Welche Branches fehlen?)
- [ ] Dependency-Analyse (veraltete Packages, Security-Alerts)

**Phase 3: Automatisierung**
- [ ] Auto-Fix f√ºr einfache Issues (z.B. Docstrings, Type-Hints)
- [ ] Test-Generierung f√ºr neue Funktionen
- [ ] Dokumentations-Generierung f√ºr APIs
- [ ] Code-Completion (kontextbezogen)

**Phase 4: Integration**
- [ ] IDE-Plugin (VS Code, Cursor)
- [ ] CI/CD-Integration (GitHub Actions, GitLab CI)
- [ ] Dashboard (Metriken, Trends, False-Positive-Rate)
- [ ] Feedback-Loop (Verbesserung durch User-Feedback)

---

### üìö Weitere Ressourcen

- **Prompt-Templates:** `docs/ki/LLM_CODE_REVIEW_PROMPTS.md` (TODO)
- **Schema-Definitionen:** `backend/models/llm_schemas.py` (TODO)
- **CI/CD-Scripts:** `scripts/llm_code_review.py` (TODO)
- **Metriken-Dashboard:** `/admin/llm-metrics` (TODO)

---

## Coding Standards

### Python

- **Version**: Python ‚â•3.11
- **Pydantic**: v2 f√ºr Datenvalidierung
- **FastAPI**: F√ºr REST-APIs
- **Zeitzone**: `TZ=UTC` in allen Services
- **Locale**: `LC_ALL=C.UTF-8` in allen Services

### Code-Qualit√§t

- **Keine globalen Zust√§nde**: Repos/Services als Konstruktor-Dependencies
- **Konfiguration**: Nur via ENV (12-Factor): `ENGINE_VERSION|RULESET_VERSION|REPAIR_VERSION|ROUTER_URL`
- **HTTP**: Zeitouts/Retry/Circuit-Breaker zentral in Client; keine Ad-hoc-Requests
- **Fehlerbehandlung**: 4xx f√ºr Userfehler, 5xx f√ºr Systemfehler; niemals 200 bei Fehlvalidierung
- **Logging**: Strukturiert (JSON), Felder: `correlation_id`, `tour_uid`, `stop_uid`, `phase`, `latency_ms`
- **Dependencies**: Versionsfix (`==`), kein Sniffing (CSV), kein `random` ohne Seed

### Encoding-Kontrakt

- **Lesen**: Heuristisch (cp850 / utf-8-sig / latin-1)
- **Schreiben/Export/Logs**: **Immer UTF-8**

### Unantastbare Bereiche

- `./Tourplaene/**` (Originale)
- `tools/orig_integrity.py`, `ingest/reader.py`
- Keine √Ñnderungen durch Prompts ohne explizite Freigabe

---

## Architektur-Prinzipien

### Grundprinzipien

1. **Determinismus**: Gleicher Input ‚áí gleicher Output
   - Keine Zufallsquellen, keine kontextabh√§ngigen Zeiten
   - Sortierung und Tie-Breaker festlegen

2. **Vertragstreue**: Eingehendes Format bleibt stabil
   - Keine √Ñnderung/Umbenennung von Feldern upstream

3. **Defense-in-Depth**: 
   - Blacklist ‚Üí Exact ‚Üí Regex ‚Üí Gazetteer/Postal ‚Üí Rules ‚Üí LLM (nur als Fallback)

4. **Transparenz**: 
   - Jede √Ñnderung durch Events/Metriken belegbar (Audit-Log, Stats-API)
   - Kein "silent fix-up"

5. **Sicherheitsgurt**: 
   - Fehler ‚áí Quarant√§ne/HTTP-4xx, nicht heuristisch weiterrechnen

### Schichtarchitektur

```
Frontend (HTML/CSS/JS)
    ‚Üì
API Layer (FastAPI Routes)
    ‚Üì
Business Logic (Services)
    ‚Üì
Data Access (Repositories)
    ‚Üì
Database (SQLite/PostgreSQL)
```

### Modulare Struktur

- **Backend**: `backend/` - FastAPI Backend
- **Frontend**: `frontend/` - HTML/CSS/JS Frontend
- **Services**: `services/` - Business Logic
- **Repositories**: `repositories/` - Data Access
- **Database**: `db/` - Schema & Migrations
- **Scripts**: `scripts/` - Utility-Scripts
- **Tests**: `tests/` - Unit-Tests
- **Tools**: `tools/` - Development Tools

---

## API-Standards

### REST-Konventionen

- **GET**: Lesen (idempotent)
- **POST**: Erstellen/Ausf√ºhren (nicht idempotent)
- **PUT**: Vollst√§ndiges Update (idempotent)
- **PATCH**: Teilweises Update (idempotent)
- **DELETE**: L√∂schen (idempotent)

### Response-Format

```json
{
  "success": true,
  "data": { ... },
  "meta": {
    "timestamp": "2025-11-13T14:00:00Z",
    "version": "1.0.0"
  }
}
```

### Fehlerbehandlung

```json
{
  "success": false,
  "error": {
    "code": "VALIDATION_ERROR",
    "message": "Fehlerbeschreibung",
    "details": { ... }
  }
}
```

### HTTP-Status-Codes

- **200**: Erfolg
- **201**: Erstellt
- **400**: Client-Fehler (Validierung)
- **401**: Nicht authentifiziert
- **403**: Nicht autorisiert
- **404**: Nicht gefunden
- **422**: Unprocessable Entity (Validierungsfehler)
- **500**: Server-Fehler
- **503**: Service nicht verf√ºgbar

### API-Dokumentation

- **OpenAPI/Swagger**: Automatisch generiert via FastAPI
- **Endpoint**: `/docs` (Swagger UI), `/openapi.json` (OpenAPI Schema)
- **Beispiele**: Jeder Endpoint sollte Beispiele enthalten

---

## Testing-Standards

### Test-Typen

1. **Unit-Tests**: Einzelne Funktionen/Klassen
2. **Integration-Tests**: Komponenten-Interaktion
3. **E2E-Tests**: Vollst√§ndige Workflows
4. **Golden-Tests**: Problemf√§lle (z.B. spezielle Adressen)
5. **Property-Tests**: Idempotenz, Set-Gleichheit
6. **Snapshot-Tests**: Optimize-Antworten (mit fixen Seeds)

### Coverage-Anforderungen

- **Minimum**: 80% Code-Coverage
- **Kritische Pfade**: 100% Coverage
- **CI-Fail**: Wenn Coverage unter 80% f√§llt

### Test-Struktur

```
tests/
‚îú‚îÄ‚îÄ unit/           # Unit-Tests
‚îú‚îÄ‚îÄ integration/    # Integration-Tests
‚îú‚îÄ‚îÄ e2e/            # End-to-End-Tests
‚îî‚îÄ‚îÄ fixtures/       # Test-Daten
```

### Test-Ausf√ºhrung

```bash
# Alle Tests
pytest

# Mit Coverage
pytest --cov=backend --cov=repositories --cov=services

# Spezifische Tests
pytest tests/test_geocode_robust_simple.py -v
```

### Pre-commit-Hooks

- **Lint**: `ruff check`
- **Type-Check**: `mypy`
- **Tests**: `pytest` (schnelle Tests)
- **Format**: `ruff format`

---

## Git & Versionierung

### Branch-Strategie

- **main/master**: Produktions-Branch (immer deploybar)
- **develop**: Entwicklungs-Branch
- **feature/**: Feature-Entwicklung (`feature/engine-optimization`)
- **fix/**: Bugfixes (`fix/osrm-timeout`)
- **chore/**: Wartungsarbeiten (`chore/telemetry-update`)
- **governance/**: Governance-√Ñnderungen (`governance/cursor-rules`)

### Commit-Messages (Conventional Commits)

```
feat: Neue Feature-Beschreibung
fix: Bugfix-Beschreibung
docs: Dokumentations-√Ñnderung
test: Test-√Ñnderung
refactor: Code-Refactoring
chore: Wartungsarbeit
```

### PR-Prozess

**PR-Checklist:**
- [ ] Keine √Ñnderungen an unantastbaren Bereichen
- [ ] API-Kontrakte unver√§ndert (oder Migrationsnotiz enthalten)
- [ ] Tests gr√ºn (Golden/Property/Snapshot) & Coverage ‚â• 80%
- [ ] Timeouts/Retry/Circuit-Breaker konfiguriert
- [ ] LLM-Pfad strikt validiert (falls verwendet)
- [ ] Metriken & Logs erweitert
- [ ] Dokumentation aktualisiert

### CI/CD-Pipeline

- **Pre-commit**: Lint, Format, Type-Check
- **CI**: Tests, Coverage, Docker-Build
- **CD**: Automatisches Deployment (nach Review)

---

## Deployment & Operations

### Umgebungsvariablen (12-Factor)

Alle Konfiguration via ENV:
- `DATABASE_URL`
- `OSRM_BASE_URL`
- `OPENAI_API_KEY` (falls verwendet)
- `APP_ENV` (dev/staging/prod)
- `LOG_LEVEL`

### Docker

- **Dockerfile**: Multi-Stage Build
- **docker-compose.yml**: Service-Orchestrierung
- **Read-Only Mounts**: Original-Verzeichnisse read-only

### Logging

- **Format**: Strukturiert (JSON)
- **Levels**: DEBUG, INFO, WARNING, ERROR, CRITICAL
- **Felder**: `timestamp`, `level`, `message`, `correlation_id`, `context`
- **Ausgabe**: Console + File (rotierend)

### Monitoring

- **Health-Checks**: `/health/db`, `/health/osrm`
- **Metriken**: Prometheus-kompatibel
- **Alarme**: 
  - `osrm_unavailable > 0` in 5min ‚áí Warnung
  - `llm_invalid_schema > 0` ‚áí Review
  - `tours_pending_geo` steigt 3 Intervalle ‚áí GeoQueue pr√ºfen

---

## Audit & Compliance

### ‚≠ê KI-Audit-Framework (PRIM√ÑR)

**Ziel**: Strukturierte, reproduzierbare, ganzheitliche Code-Audits mit Cursor AI

**Zentrale Dokumentation**: **`docs/ki/`**

| Dokument | Zweck |
|----------|-------|
| **[ki/README.md](ki/README.md)** | Framework-√úbersicht & Workflow |
| **[ki/REGELN_AUDITS.md](ki/REGELN_AUDITS.md)** | Grundregeln f√ºr alle Audits |
| **[ki/AUDIT_CHECKLISTE.md](ki/AUDIT_CHECKLISTE.md)** | 9-Punkte-Checkliste |
| **[ki/LESSONS_LOG.md](ki/LESSONS_LOG.md)** | Dokumentierte Fehler & L√∂sungen |
| **[ki/CURSOR_PROMPT_TEMPLATE.md](ki/CURSOR_PROMPT_TEMPLATE.md)** | 10 fertige Prompts |

**Quick-Referenz**: [`KI_AUDIT_FRAMEWORK.md`](../KI_AUDIT_FRAMEWORK.md) (Projekt-Root)

**Alle Code-Reviews und Audits folgen diesem Framework!**

### Code-Audit Playbook (Legacy)

**Hinweis**: Wurde durch KI-Audit-Framework ersetzt. F√ºr Altprojekte:

**Dokumentation**: Siehe **[CODE_AUDIT_PLAYBOOK.md](STANDARDS/CODE_AUDIT_PLAYBOOK.md)** f√ºr:
- Standard-Audit-Reihenfolge
- Fix-Vorschl√§ge (Middleware, Frontend, Statuscodes, OSRM, DB)
- Cursor-Ablauf (deterministisch)
- PR-Template
- Artefakte-Packaging

### Audit-ZIP-Pipeline

**Ziel**: Ein Klick, ein ZIP. Alle audit-relevanten Dateien landen konsistent in `ZIP/` als `AUDIT_<YYYYMMDD_HHMMSS>_<shortsha>.zip`

**Verwendung**:
- **Linux/macOS**: `bash scripts/make_audit_zip.sh`
- **Windows**: `pwsh -File scripts/Make-AuditZip.ps1`
- **Direkt**: `python tools/make_audit_zip.py`

**Enthalten**:
- Manifest (Hashes, Commit, Branch)
- Logs, OpenAPI, Routenliste
- Sanitizierte `.env` (Secrets redacted)

**Details**: Siehe `tools/make_audit_zip.py`

### Secrets-Schutz

- **.env ‚Üí .env.audit**: Ersetze Werte folgender Keys durch Platzhalter:
  - `OPENAI_API_KEY`, `DATABASE_URL`, `POSTGRES_PASSWORD`, `REDIS_URL`, `SMTP_PASSWORD`, `API_KEY`, `SECRET`, `TOKEN`
- **Nicht-geheim & hilfreich bleiben drin**: `OSRM_URL`, `OSRM_TIMEOUT`, `APP_ENV`

### Integrit√§tspr√ºfung

- **SHA256-Hashes**: F√ºr Original-Dateien
- **Pre-commit-Hooks**: Schutz vor versehentlichen √Ñnderungen
- **CI/CD**: Automatische Validierung bei jedem Push/PR

---

## Dokumentations-Standards

### Struktur

```
docs/
‚îú‚îÄ‚îÄ STANDARDS.md           # Diese Datei (Zentrale Standards)
‚îú‚îÄ‚îÄ README.md              # Projekt-README
‚îú‚îÄ‚îÄ ARCHITECTURE.md        # System-Architektur
‚îú‚îÄ‚îÄ API.md                 # API-Dokumentation
‚îú‚îÄ‚îÄ DEVELOPMENT.md         # Entwickler-Guide
‚îú‚îÄ‚îÄ DEPLOYMENT.md          # Deployment-Guide
‚îî‚îÄ‚îÄ CHANGELOG.md           # √Ñnderungsprotokoll
```

### Dokumentations-Prinzipien

1. **Aktuell halten**: Dokumentation muss mit Code synchronisiert sein
2. **Beispiele**: Jede Funktion sollte Beispiele enthalten
3. **Strukturiert**: Klare Gliederung, Inhaltsverzeichnis
4. **Wiederverwendbar**: Standards f√ºr alle Projekte

### Markdown-Standards

- **√úberschriften**: H1 f√ºr Titel, H2 f√ºr Hauptabschnitte, H3 f√ºr Unterabschnitte
- **Code-Bl√∂cke**: Mit Sprach-Tag (`python`, `bash`, `json`)
- **Links**: Relative Links zu anderen Dokumenten
- **Tabellen**: F√ºr strukturierte Daten

### Changelog

- **Format**: [Keep a Changelog](https://keepachangelog.com/)
- **Kategorien**: Added, Changed, Deprecated, Removed, Fixed, Security
- **Datum**: ISO-Format (YYYY-MM-DD)

---

## Definition of Done (DoD)

Ein Feature ist "Done", wenn:

- [ ] Code implementiert und getestet
- [ ] Unit-Tests geschrieben (Coverage ‚â• 80%)
- [ ] Integration-Tests bestanden
- [ ] Dokumentation aktualisiert
- [ ] Code-Review durchgef√ºhrt
- [ ] CI/CD-Pipeline gr√ºn
- [ ] Pre-commit-Hooks bestanden
- [ ] Metriken & Logs erweitert
- [ ] Keine Breaking Changes (oder Migrationsnotiz)

---

## Verbote (Anti-Anarchie-Liste)

- ‚ùå Keine √Ñnderungen an unantastbaren Bereichen ohne explizite Freigabe
- ‚ùå Kein Index-Mapping/Koordinatenvergleich als Identit√§t ‚Äì nur UIDs
- ‚ùå Kein LLM ohne Schema/Validierung/Verifikation
- ‚ùå Keine externen HTTP-Calls ohne zentralen Client/Timeout/Retry
- ‚ùå Keine "silent fixes" ‚Äì Fehler m√ºssen sichtbar/quittiert sein
- ‚ùå Keine globalen Zust√§nde
- ‚ùå Keine Ad-hoc-Requests im Codepfad
- ‚ùå Keine 200-Responses bei Fehlvalidierung

---

## Weiterf√ºhrende Dokumentation

### KI-Audit-Framework (PRIM√ÑR) ‚≠ê

- **Framework-√úbersicht**: `docs/ki/README.md` ‚≠ê **NEU**
- **Audit-Grundregeln**: `docs/ki/REGELN_AUDITS.md` ‚≠ê **NEU**
- **Audit-Checkliste**: `docs/ki/AUDIT_CHECKLISTE.md` ‚≠ê **NEU**
- **Lessons Learned**: `docs/ki/LESSONS_LOG.md` ‚≠ê **NEU**
- **Cursor-Prompts**: `docs/ki/CURSOR_PROMPT_TEMPLATE.md` ‚≠ê **NEU**
- **Quick-Referenz**: `KI_AUDIT_FRAMEWORK.md` (Projekt-Root) ‚≠ê **NEU**

### Cursor AI

- **Cursor KI Betriebsordnung**: `docs/CURSOR_KI_BETRIEBSORDNUNG.md`
- **Cursor Arbeitsrichtlinie**: `docs/Cursor-Arbeitsrichtlinie.md`

### Code & Architektur

- **Code-Audit Playbook (Legacy)**: `docs/STANDARDS/CODE_AUDIT_PLAYBOOK.md`
- **Architektur**: `docs/Architecture.md`
- **API-Dokumentation**: `docs/Api_Docs.md`
- **Developer Guide**: `docs/DEVELOPER_GUIDE.md`

---

## Changelog

### Version 2.0 (2025-11-14) ‚≠ê

**BREAKING CHANGE: KI-Audit-Framework ist jetzt PFLICHT!**

- ‚úÖ **NEU:** Vollst√§ndiges KI-Audit-Framework in `docs/ki/`
- ‚úÖ **NEU:** Ganzheitliche Code-Reviews (Backend + Frontend + DB + Infra)
- ‚úÖ **NEU:** 7 unverhandelbare Audit-Regeln
- ‚úÖ **NEU:** 10 fertige Cursor-Prompts f√ºr verschiedene Audit-Szenarien
- ‚úÖ **NEU:** LESSONS_LOG f√ºr dokumentierte Fehler & L√∂sungen
- ‚úÖ **NEU:** Strukturierter 6-Phasen-Audit-Workflow
- ‚úÖ **NEU:** Code-Review-Standards (Backend + Frontend gemeinsam!)
- ‚úÖ **√ÑNDERUNG:** Alle Audits und Code-Reviews m√ºssen ganzheitlich sein
- ‚úÖ **√ÑNDERUNG:** Isolierte Fixes sind verboten
- ‚úÖ **√ÑNDERUNG:** Tests sind Pflicht (mindestens 1 Regressionstest pro Fix)

**Migration:**
- Alle neuen Audits: Folgen Sie `docs/ki/REGELN_AUDITS.md`
- Alle Code-Reviews: Pr√ºfen Sie Backend + Frontend + DB + Infrastruktur
- Bei Bugs: Schreiben Sie einen Regressionstest
- Bei neuen Fehlertypen: Aktualisieren Sie `docs/ki/LESSONS_LOG.md`

### Version 1.0 (2025-11-13)

- Initiale Version der zentralen Standards-Dokumentation
- Zusammenf√ºhrung aller Best Practices

---

**Diese Standards gelten f√ºr alle FAMO-Projekte und sind verbindlich.**

**Ab Version 2.0: KI-Audit-Framework ist PFLICHT f√ºr alle Code-Reviews und Audits!**

