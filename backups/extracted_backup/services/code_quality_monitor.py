# -*- coding: utf-8 -*-
"""
Code-Quality-Monitoring Service für FAMO TrafficApp 3.0

Überwacht Code-Qualität und erkennt Änderungen durch Cursor-KI.
"""

import os
import ast
import logging
import subprocess
import json
from datetime import datetime
from typing import Dict, List, Any, Optional, Tuple
from pathlib import Path
from dataclasses import dataclass
import difflib

@dataclass
class CodeQualityReport:
    """Report für Code-Qualität"""
    file_path: str
    timestamp: str
    lines_of_code: int
    complexity_score: float
    issues: List[Dict[str, Any]]
    ai_indicators: List[str]
    quality_score: float

@dataclass
class DiffAnalysis:
    """Analyse von Code-Änderungen"""
    file_path: str
    changes_count: int
    added_lines: int
    removed_lines: int
    ai_patterns: List[str]
    quality_impact: str
    risk_level: str

class CodeQualityMonitor:
    """Monitor für Code-Qualität und KI-Änderungen"""
    
    def __init__(self, project_root: str = "."):
        self.project_root = Path(project_root)
        self.logger = logging.getLogger(__name__)
        
        # AI-Indikatoren für Code-Erkennung
        self.ai_patterns = {
            "comments": [
                "generated by",
                "ai generated",
                "cursor generated",
                "automatically generated",
                "llm generated"
            ],
            "code_styles": [
                "overly_verbose",
                "unnecessary_complexity",
                "generic_variable_names",
                "excessive_documentation"
            ],
            "imports": [
                "unused_imports",
                "wildcard_imports",
                "circular_imports"
            ]
        }
        
        # Qualitätsregeln
        self.quality_rules = {
            "max_function_length": 50,
            "max_class_length": 200,
            "max_complexity": 10,
            "max_parameters": 5,
            "min_test_coverage": 0.8
        }
    
    def analyze_file(self, file_path: str) -> CodeQualityReport:
        """
        Analysiert Code-Qualität einer Datei
        
        Args:
            file_path: Pfad zur Python-Datei
            
        Returns:
            CodeQualityReport mit Analyse-Ergebnissen
        """
        file_path = Path(file_path)
        
        if not file_path.exists() or file_path.suffix != '.py':
            raise ValueError(f"Invalid Python file: {file_path}")
        
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # Parse AST
            tree = ast.parse(content)
            
            # Analysiere Code
            lines_of_code = len(content.splitlines())
            complexity_score = self._calculate_complexity(tree)
            issues = self._detect_issues(tree, content)
            ai_indicators = self._detect_ai_patterns(content)
            quality_score = self._calculate_quality_score(lines_of_code, complexity_score, issues)
            
            return CodeQualityReport(
                file_path=str(file_path),
                timestamp=datetime.now().isoformat(),
                lines_of_code=lines_of_code,
                complexity_score=complexity_score,
                issues=issues,
                ai_indicators=ai_indicators,
                quality_score=quality_score
            )
            
        except Exception as e:
            self.logger.error(f"Error analyzing file {file_path}: {e}")
            raise
    
    def _calculate_complexity(self, tree: ast.AST) -> float:
        """Berechnet Komplexitäts-Score"""
        complexity = 0
        
        for node in ast.walk(tree):
            if isinstance(node, (ast.If, ast.While, ast.For, ast.AsyncFor)):
                complexity += 1
            elif isinstance(node, ast.ExceptHandler):
                complexity += 1
            elif isinstance(node, ast.BoolOp):
                complexity += len(node.values) - 1
        
        return complexity
    
    def _detect_issues(self, tree: ast.AST, content: str) -> List[Dict[str, Any]]:
        """Erkennt Code-Qualitätsprobleme"""
        issues = []
        
        # Funktionen zu lang
        for node in ast.walk(tree):
            if isinstance(node, ast.FunctionDef):
                func_lines = node.end_lineno - node.lineno if hasattr(node, 'end_lineno') else 1
                if func_lines > self.quality_rules["max_function_length"]:
                    issues.append({
                        "type": "function_too_long",
                        "severity": "warning",
                        "line": node.lineno,
                        "message": f"Function '{node.name}' is {func_lines} lines long (max: {self.quality_rules['max_function_length']})"
                    })
                
                # Zu viele Parameter
                if len(node.args.args) > self.quality_rules["max_parameters"]:
                    issues.append({
                        "type": "too_many_parameters",
                        "severity": "warning",
                        "line": node.lineno,
                        "message": f"Function '{node.name}' has {len(node.args.args)} parameters (max: {self.quality_rules['max_parameters']})"
                    })
        
        # Klassen zu lang
        for node in ast.walk(tree):
            if isinstance(node, ast.ClassDef):
                class_lines = node.end_lineno - node.lineno if hasattr(node, 'end_lineno') else 1
                if class_lines > self.quality_rules["max_class_length"]:
                    issues.append({
                        "type": "class_too_long",
                        "severity": "warning",
                        "line": node.lineno,
                        "message": f"Class '{node.name}' is {class_lines} lines long (max: {self.quality_rules['max_class_length']})"
                    })
        
        # Unbenutzte Imports
        imports = [node for node in ast.walk(tree) if isinstance(node, ast.Import)]
        import_froms = [node for node in ast.walk(tree) if isinstance(node, ast.ImportFrom)]
        
        # Vereinfachte Implementierung - in Produktion erweitern
        for imp in imports + import_froms:
            if hasattr(imp, 'names'):
                for alias in imp.names:
                    if alias.name.startswith('_'):
                        issues.append({
                            "type": "unused_import",
                            "severity": "info",
                            "line": imp.lineno,
                            "message": f"Potentially unused import: {alias.name}"
                        })
        
        return issues
    
    def _detect_ai_patterns(self, content: str) -> List[str]:
        """Erkennt AI-generierte Code-Patterns"""
        ai_indicators = []
        content_lower = content.lower()
        
        # Kommentar-Patterns
        for pattern in self.ai_patterns["comments"]:
            if pattern in content_lower:
                ai_indicators.append(f"ai_comment_pattern: {pattern}")
        
        # Code-Style-Patterns
        lines = content.splitlines()
        
        # Übermäßig ausführliche Kommentare
        comment_ratio = sum(1 for line in lines if line.strip().startswith('#')) / len(lines) if lines else 0
        if comment_ratio > 0.3:
            ai_indicators.append("excessive_comments")
        
        # Generische Variablennamen
        generic_names = ['data', 'result', 'temp', 'value', 'item', 'obj']
        for line in lines:
            for name in generic_names:
                if f" = {name}" in line or f"({name})" in line:
                    ai_indicators.append(f"generic_variable_name: {name}")
                    break
        
        # Übermäßig lange Zeilen
        long_lines = sum(1 for line in lines if len(line) > 120)
        if long_lines > len(lines) * 0.1:
            ai_indicators.append("excessive_long_lines")
        
        return ai_indicators
    
    def _calculate_quality_score(self, lines_of_code: int, complexity: float, issues: List[Dict]) -> float:
        """Berechnet Gesamt-Qualitäts-Score (0-1)"""
        score = 1.0
        
        # Komplexitäts-Penalty
        if complexity > self.quality_rules["max_complexity"]:
            score -= 0.2
        
        # Issues-Penalty
        critical_issues = sum(1 for issue in issues if issue.get("severity") == "critical")
        warning_issues = sum(1 for issue in issues if issue.get("severity") == "warning")
        
        score -= critical_issues * 0.1
        score -= warning_issues * 0.05
        
        # Größen-Penalty
        if lines_of_code > 500:
            score -= 0.1
        
        return max(0.0, min(1.0, score))
    
    def analyze_diff(self, old_content: str, new_content: str, file_path: str) -> DiffAnalysis:
        """
        Analysiert Unterschiede zwischen Code-Versionen
        
        Args:
            old_content: Alter Code
            new_content: Neuer Code
            file_path: Dateipfad
            
        Returns:
            DiffAnalysis mit Änderungsanalyse
        """
        old_lines = old_content.splitlines()
        new_lines = new_content.splitlines()
        
        # Berechne Diff-Statistiken
        differ = difflib.unified_diff(old_lines, new_lines, lineterm='')
        changes = list(differ)
        
        added_lines = sum(1 for line in changes if line.startswith('+'))
        removed_lines = sum(1 for line in changes if line.startswith('-'))
        changes_count = added_lines + removed_lines
        
        # Erkenne AI-Patterns in Änderungen
        ai_patterns = []
        for line in changes:
            if line.startswith('+'):
                line_content = line[1:].lower()
                for pattern in self.ai_patterns["comments"]:
                    if pattern in line_content:
                        ai_patterns.append(f"ai_pattern_in_addition: {pattern}")
        
        # Bewerte Qualitäts-Impact
        quality_impact = self._assess_quality_impact(old_content, new_content)
        
        # Bestimme Risiko-Level
        risk_level = self._assess_risk_level(changes_count, ai_patterns, quality_impact)
        
        return DiffAnalysis(
            file_path=file_path,
            changes_count=changes_count,
            added_lines=added_lines,
            removed_lines=removed_lines,
            ai_patterns=ai_patterns,
            quality_impact=quality_impact,
            risk_level=risk_level
        )
    
    def _assess_quality_impact(self, old_content: str, new_content: str) -> str:
        """Bewertet Qualitäts-Impact der Änderungen"""
        try:
            old_tree = ast.parse(old_content)
            new_tree = ast.parse(new_content)
            
            old_complexity = self._calculate_complexity(old_tree)
            new_complexity = self._calculate_complexity(new_tree)
            
            if new_complexity > old_complexity * 1.5:
                return "negative"
            elif new_complexity < old_complexity * 0.8:
                return "positive"
            else:
                return "neutral"
                
        except SyntaxError:
            return "syntax_error"
    
    def _assess_risk_level(self, changes_count: int, ai_patterns: List[str], quality_impact: str) -> str:
        """Bestimmt Risiko-Level der Änderungen"""
        risk_score = 0
        
        # Anzahl Änderungen
        if changes_count > 100:
            risk_score += 3
        elif changes_count > 50:
            risk_score += 2
        elif changes_count > 10:
            risk_score += 1
        
        # AI-Patterns
        risk_score += len(ai_patterns)
        
        # Qualitäts-Impact
        if quality_impact == "negative":
            risk_score += 2
        elif quality_impact == "syntax_error":
            risk_score += 3
        
        # Risiko-Level bestimmen
        if risk_score >= 5:
            return "high"
        elif risk_score >= 3:
            return "medium"
        else:
            return "low"
    
    def run_linter_checks(self, file_path: str) -> Dict[str, Any]:
        """Führt Linter-Checks aus"""
        results = {
            "ruff": self._run_ruff(file_path),
            "mypy": self._run_mypy(file_path),
            "pylint": self._run_pylint(file_path)
        }
        
        return results
    
    def _run_ruff(self, file_path: str) -> Dict[str, Any]:
        """Führt Ruff-Linter aus"""
        try:
            result = subprocess.run(
                ["ruff", "check", file_path],
                capture_output=True,
                text=True,
                timeout=30
            )
            
            return {
                "success": result.returncode == 0,
                "output": result.stdout,
                "errors": result.stderr,
                "exit_code": result.returncode
            }
        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "output": "",
                "errors": str(e)
            }
    
    def _run_mypy(self, file_path: str) -> Dict[str, Any]:
        """Führt MyPy Type-Checker aus"""
        try:
            result = subprocess.run(
                ["mypy", file_path],
                capture_output=True,
                text=True,
                timeout=30
            )
            
            return {
                "success": result.returncode == 0,
                "output": result.stdout,
                "errors": result.stderr,
                "exit_code": result.returncode
            }
        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "output": "",
                "errors": str(e)
            }
    
    def _run_pylint(self, file_path: str) -> Dict[str, Any]:
        """Führt Pylint aus"""
        try:
            result = subprocess.run(
                ["pylint", file_path],
                capture_output=True,
                text=True,
                timeout=30
            )
            
            return {
                "success": result.returncode == 0,
                "output": result.stdout,
                "errors": result.stderr,
                "exit_code": result.returncode
            }
        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "output": "",
                "errors": str(e)
            }
    
    def generate_quality_report(self, directory: str = ".") -> Dict[str, Any]:
        """Generiert Qualitäts-Report für Verzeichnis"""
        directory = Path(directory)
        python_files = list(directory.rglob("*.py"))
        
        reports = []
        total_issues = 0
        total_ai_indicators = 0
        avg_quality_score = 0
        
        for py_file in python_files:
            try:
                report = self.analyze_file(str(py_file))
                reports.append(report)
                total_issues += len(report.issues)
                total_ai_indicators += len(report.ai_indicators)
                avg_quality_score += report.quality_score
            except Exception as e:
                self.logger.error(f"Error analyzing {py_file}: {e}")
        
        if reports:
            avg_quality_score /= len(reports)
        
        return {
            "timestamp": datetime.now().isoformat(),
            "directory": str(directory),
            "total_files": len(python_files),
            "analyzed_files": len(reports),
            "total_issues": total_issues,
            "total_ai_indicators": total_ai_indicators,
            "average_quality_score": avg_quality_score,
            "reports": [report.__dict__ for report in reports]
        }
