# Performance-Optimierung: Allow-Liste & API-Konfiguration

**Datum:** 2025-01-09  
**Status:** ‚úÖ Implementiert

---

## √úbersicht

Die **Allow-Liste** dient als **Performance-Boost**: Nur Touren in der Allow-Liste werden verarbeitet, alle anderen werden √ºbersprungen. Das beschleunigt die Verarbeitung erheblich.

**Wichtig:** Das System verwendet **OpenAI API** (Cloud-basiert), **keine lokalen Modelle**. Dadurch:
- ‚úÖ Funktioniert auf jedem Rechner (Laptop, Desktop, Server)
- ‚úÖ Keine GPU erforderlich
- ‚úÖ Nur Internet-Verbindung n√∂tig
- ‚úÖ Skalierbar und schnell

---

## Allow-Liste als Speed-Boost

### Konzept

**Datei:** `config/tour_ignore_list.json`

```json
{
  "allow_tours": ["CB", "T", "BZ"]
}
```

**Effekt:**
- Nur CB, T, BZ Touren werden verarbeitet
- Alle anderen Touren (W, PIR, FG, etc.) werden √ºbersprungen
- **Resultat:** Deutlich weniger Touren = viel schneller!

### Performance-Gewinn

**Beispiel-Szenario:**
- **Vorher:** 20 Touren verarbeiten (alle)
- **Nachher:** 5 Touren verarbeiten (nur CB, T, BZ)
- **Gewinn:** ~75% weniger Verarbeitung = 4x schneller

**F√ºr Single-Tour-Routen (FG, etc.):**
- Diese k√∂nnen in Allow-Liste **nicht** stehen
- Werden automatisch √ºbersprungen
- System konzentriert sich nur auf relevante Touren

---

## API-Konfiguration (Keine lokalen Modelle!)

### Aktuelles System

**Haupt-LLM:** `services/llm_optimizer.py`
- ‚úÖ Verwendet **nur OpenAI API**
- ‚úÖ Model: `gpt-4o-mini` (schnell, kosteng√ºnstig)
- ‚úÖ Keine lokalen Modelle
- ‚úÖ Keine GPU erforderlich

**Konfiguration:**
```python
self.client = openai.OpenAI(api_key=self.api_key)
self.model = "gpt-4o-mini"  # Cloud-basiert, keine GPU n√∂tig
```

### Alternative: `backend/services/ai_optimizer.py`

**‚ö†Ô∏è WARNUNG:** Diese Klasse hat `use_local` Parameter!

**Um sicherzustellen dass nur API verwendet wird:**

**Option 1:** Code pr√ºfen wo `AIOptimizer` initialisiert wird:
```python
# ‚ùå FALSCH (verwendet lokale Modelle):
optimizer = AIOptimizer(use_local=True)

# ‚úÖ RICHTIG (verwendet nur API):
optimizer = AIOptimizer(use_local=False, api_key=os.getenv("OPENAI_API_KEY"))
```

**Option 2:** Nur `LLMOptimizer` verwenden (bereits API-only):
```python
from services.llm_optimizer import LLMOptimizer
optimizer = LLMOptimizer()  # ‚úÖ Automatisch API-only
```

---

## Performance-Tipps

### 1. Allow-Liste gezielt einsetzen

**F√ºr schnelle Verarbeitung:**
```json
{
  "allow_tours": ["CB", "T", "BZ"]  // Nur direkte Routen
}
```

**F√ºr alle Touren:**
```json
{
  "allow_tours": []  // Leer = alle erlauben
}
```

### 2. Ignore-Liste f√ºr Pickups/Nachtlieferungen

```json
{
  "ignore_tours": ["DBD", "DPD", "DVD"]  // √úberspringen, nicht relevant
}
```

### 3. API-Key sicher konfigurieren

**.env Datei:**
```env
OPENAI_API_KEY=sk-proj-...
LLM_MODEL=gpt-4o-mini  # Schnell & g√ºnstig
```

**Verifizierung:**
```python
import os
api_key = os.getenv("OPENAI_API_KEY")
if not api_key:
    print("‚ö†Ô∏è WARNUNG: OPENAI_API_KEY nicht gesetzt!")
```

### 4. Keine lokalen Modelle laden

**Entfernen/Fertigstellen wenn vorhanden:**
- `ai_models/` Ordner (falls vorhanden)
- `backend/services/ai_optimizer.py` ‚Üí `use_local=False` setzen
- Ollama-Integration deaktivieren

---

## Geschwindigkeits-Vergleich

### Mit Allow-Liste (CB, T, BZ)
```
‚úÖ 5 Touren verarbeiten
‚è±Ô∏è ~30 Sekunden
üíª Funktioniert auf jedem Rechner (API-only)
```

### Ohne Allow-Liste (alle Touren)
```
‚ö†Ô∏è 20 Touren verarbeiten
‚è±Ô∏è ~2 Minuten
üíª Funktioniert auf jedem Rechner (API-only)
```

**Gewinn:** ~4x schneller mit Allow-Liste!

---

## Laptop-Performance

### Warum ist es auf Laptop langsam?

**M√∂gliche Ursachen:**
1. ‚ö†Ô∏è Lokale Modelle werden verwendet (brauchen GPU)
2. ‚ö†Ô∏è Allow-Liste ist nicht aktiviert (alle Touren werden verarbeitet)
3. ‚ö†Ô∏è Langsame Internet-Verbindung (API-Calls dauern lange)

### L√∂sung

1. **Verifiziere API-Only:**
   ```python
   # In workflow_api.py pr√ºfen:
   llm_optimizer = LLMOptimizer()
   print(f"LLM enabled: {llm_optimizer.enabled}")  # Sollte True sein
   print(f"API Key vorhanden: {bool(llm_optimizer.api_key)}")  # Sollte True sein
   ```

2. **Aktiviere Allow-Liste:**
   ```json
   {
     "allow_tours": ["CB", "T", "BZ"]  // Nur diese verarbeiten
   }
   ```

3. **Internet-Verbindung pr√ºfen:**
   ```bash
   curl https://api.openai.com/v1/models
   ```

---

## Checkliste f√ºr optimale Performance

- [ ] Allow-Liste aktiviert (`allow_tours` nicht leer, nur relevante Touren)
- [ ] Ignore-Liste konfiguriert (DBD, DPD, DVD)
- [ ] OpenAI API-Key gesetzt (`OPENAI_API_KEY` in `.env`)
- [ ] Nur `LLMOptimizer` verwendet (API-only, keine lokalen Modelle)
- [ ] `use_local=False` bei `AIOptimizer` (falls verwendet)
- [ ] Internet-Verbindung stabil
- [ ] Model: `gpt-4o-mini` (schnell & g√ºnstig)

---

## Debugging

### Pr√ºfe welche LLM verwendet wird

**Im Server-Log suchen:**
```
[INFO] LLM-Optimizer initialized successfully
[INFO] Model: gpt-4o-mini
```

**Wenn lokale Modelle verwendet werden:**
```
[WARN] Using local model: qwen2.5:0.5b
[WARN] Ollama URL: http://localhost:11434
```
‚Üí **Das sollte NICHT vorkommen!**

### Pr√ºfe Allow-Liste

**Im Server-Log:**
```
[WORKFLOW] Tour-Filter geladen - Ignore: ['DBD', 'DPD', 'DVD'], Allow: ['CB', 'T', 'BZ']
[WORKFLOW] Tour 'CB-08.00 Tour' verarbeitet
[WORKFLOW] Tour 'W-07.00 Uhr Tour' √ºbersprungen (Nicht in Allow-Liste: ['CB', 'T', 'BZ'])
```

---

**Letzte Aktualisierung:** 2025-01-09  
**Status:** ‚úÖ Produktiv, Performance-optimiert

